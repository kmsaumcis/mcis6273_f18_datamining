<h1 id="mcis6273-data-mining-fall-2017-prof.-maull">MCIS6273 Data Mining / Fall 2017 / Prof. Maull</h1>
<h2 id="lecture-1-class-policies-tools-and-technologies">LECTURE 1: CLASS POLICIES, TOOLS AND TECHNOLOGIES</h2>
<table>
<colgroup>
<col width="17%" />
<col width="82%" />
</colgroup>
<thead>
<tr class="header">
<th align="right">Week of 8/30</th>
<th align="left"><a href="https://github.com/kmsaumcis/mcis6273_f17_datamining/tree/master/lecture_notes/">Lecture Notes</a></th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td align="right"><strong>Content</strong></td>
<td align="left">class policies, class tools, introduction, what this course is about, data mining: tools, technologies and techniques</td>
</tr>
<tr class="even">
<td align="right"><strong>Expected<br/>Outcomes</strong></td>
<td align="left">• overview of course policies<br/>• overview of data mining concepts, algorithms, methodologies<br/>• installation of Anaconda and Python 3.6<br/>• introduction to Jupyter Notebooks<br/>• creation of Github account<br/></td>
</tr>
<tr class="odd">
<td align="right"><strong>Readings &amp;<br/>Supplemental</strong></td>
<td align="left"><strong>REQUIRED</strong><br/>» 2014. Zaki, Mohammed J and Meira Jr, Wagner; <a href="http://www.dataminingbook.info/pmwiki.php"><em>Data mining and analysis: fundamental concepts and algorithms</em></a>. → <strong>ch.1</strong><br/>» 2014. Leskovec, Jure and Rajaraman, Anand and Ullman, Jeffrey David; <a href="http://www.mmds.org/"><em>Mining of massive datasets</em></a>. → <strong>ch.1</strong><br/>» 2011. Han, Jiawei and Pei, Jian and Kamber, Micheline; <a href="https://ia800300.us.archive.org/5/items/DataMiningConceptAndTechniques2ndEdition/Data.Mining.Concepts.and.Techniques.2nd.Ed-1558609016.pdf"><em>Data mining: concepts and techniques</em></a>. → <strong>ch.1, ch.2</strong><br/><br/><strong>OPTIONAL</strong><br/>› 2012. Downey, Allen; <a href="http://www.greenteapress.com/thinkpython/thinkpython.html"><em>Think Python</em></a>. → <strong>ch.1-ch.3</strong><br/>› (website) -- 2017; <em>The Periodic Table of Data Science</em>: <a href="https://www.datacamp.com/community/blog/data-science-periodic-table#gs.TF297Gsm" class="uri">https://www.datacamp.com/community/blog/data-science-periodic-table#gs.TF297Gsm</a>. → <strong>Familiarize yourself with the entire table.</strong><br/></td>
</tr>
<tr class="even">
<td align="right"><strong>Homework</strong></td>
<td align="left"><strong>DUE:</strong> Monday, 9/6 - midnight<br/>Please see the Blackboard/<a href="https://github.com/kmsaumcis/mcis6273_f17_datamining/homework">Github repo</a> for what to turn in.</td>
</tr>
</tbody>
</table>
<hr />
<h2 id="lecture-2-data-representation-preparation-and-manipulation">LECTURE 2: DATA / REPRESENTATION, PREPARATION AND MANIPULATION</h2>
<table>
<colgroup>
<col width="17%" />
<col width="82%" />
</colgroup>
<thead>
<tr class="header">
<th align="right">Week of 9/6</th>
<th align="left"><a href="https://github.com/kmsaumcis/mcis6273_f17_datamining/tree/master/lecture_notes">Lecture Notes</a></th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td align="right"><strong>Content</strong></td>
<td align="left">introduction to core concepts in data; data types and representation of data; data formats including structured and unstructured; concepts in pre-processing data including scaling, sampling, normalizing, binning and imputing</td>
</tr>
<tr class="even">
<td align="right"><strong>Expected<br/>Outcomes</strong></td>
<td align="left">• understand data types and common formats<br/>• identify cleaning and adjusting scenarios and apply techniques appropriately<br/>• utilize and apply the appropriate Python tools (Pandas for data import and cleaning)<br/></td>
</tr>
<tr class="odd">
<td align="right"><strong>Readings &amp;<br/>Supplemental</strong></td>
<td align="left"><strong>REQUIRED</strong><br/>» 2014. Zaki, Mohammed J and Meira Jr, Wagner; <a href="http://www.dataminingbook.info/pmwiki.php"><em>Data mining and analysis: fundamental concepts and algorithms</em></a>. → <strong>ch.1</strong><br/>» 2011. Han, Jiawei and Pei, Jian and Kamber, Micheline; <a href="https://ia800300.us.archive.org/5/items/DataMiningConceptAndTechniques2ndEdition/Data.Mining.Concepts.and.Techniques.2nd.Ed-1558609016.pdf"><em>Data mining: concepts and techniques</em></a>. → <strong>ch.1, ch.2</strong><br/>» 2012. McKinney, Wes; <a href="https://github.com/wesm/pydata-book"><em>Python for data analysis: Data wrangling with Pandas, NumPy, and IPython</em></a>. → <strong>ipython/Jupyter notebooks for ch.5, ch.6 and ch.7</strong><br/>» (website) -- 2017; <em>Distance computations (scipy.spatial.distance)</em>: <a href="https://docs.scipy.org/doc/scipy/reference/spatial.distance.html" class="uri">https://docs.scipy.org/doc/scipy/reference/spatial.distance.html</a>. → <strong>euclidean, cosine, correlation, jaccard</strong><br/><br/><strong>OPTIONAL</strong><br/>› 2012. Downey, Allen; <a href="http://www.greenteapress.com/thinkpython/thinkpython.html"><em>Think Python</em></a>. → <strong>ch.1-ch.3</strong><br/>› (website) -- 2017; <em>Pandas Cookbook</em>: <a href="https://github.com/jvns/pandas-cookbook" class="uri">https://github.com/jvns/pandas-cookbook</a>. → <strong>familiarize yourself with this content of this repo</strong><br/>› (Michael Kennedy's Talk Python To Me podcast) -- 11-28-2016; <em>Episode #90: Data Wrangling with Python</em>: <a href="http://talkpythontome.fm" class="uri">http://talkpythontome.fm</a>. → <strong>listen to the entire episode</strong><br/></td>
</tr>
<tr class="even">
<td align="right"><strong>Homework</strong></td>
<td align="left"><strong>DUE:</strong> Monday, 9/18 - midnight<br/>Please see the Blackboard/<a href="https://github.com/kmsaumcis/mcis6273_f17_datamining/homework">Github repo</a> for what to turn in.</td>
</tr>
</tbody>
</table>
<hr />
<h2 id="lecture-3-data-distance-similarity-statistical-concepts">LECTURE 3: DATA / DISTANCE, SIMILARITY, STATISTICAL CONCEPTS</h2>
<table>
<colgroup>
<col width="17%" />
<col width="82%" />
</colgroup>
<thead>
<tr class="header">
<th align="right">Week of 9/13</th>
<th align="left"><a href="https://github.com/kmsaumcis/mcis6123_fa17/tree/master/lecture_notes">Lecture Notes</a></th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td align="right"><strong>Content</strong></td>
<td align="left">introduction to comparing data using common metrics; introductory concepts in disorder; introductory statistical concepts; intuitions over data dimensionality and common reduction techniques</td>
</tr>
<tr class="even">
<td align="right"><strong>Expected<br/>Outcomes</strong></td>
<td align="left">• identify common distance metrics and their appropriate contexts<br/>• understand similarity (and dissimilarity) in data<br/>• develop intuitions of statistical concepts in correlation, distributions and expect value<br/>• understand dimensionality reduction via PCA<br/>• utilize and apply basic statistical tools in Python (Pandas/Numpy)<br/></td>
</tr>
<tr class="odd">
<td align="right"><strong>Readings &amp;<br/>Supplemental</strong></td>
<td align="left"><strong>REQUIRED</strong><br/>» 2014. Zaki, Mohammed J and Meira Jr, Wagner; <a href="http://www.dataminingbook.info/pmwiki.php"><em>Data mining and analysis: fundamental concepts and algorithms</em></a>. → <strong>ch.7</strong><br/>» 2014. Leskovec, Jure and Rajaraman, Anand and Ullman, Jeffrey David; <a href="http://www.mmds.org/"><em>Mining of massive datasets</em></a>. → <strong>ch.11</strong><br/>» 2011. Han, Jiawei and Pei, Jian and Kamber, Micheline; <a href="https://ia800300.us.archive.org/5/items/DataMiningConceptAndTechniques2ndEdition/Data.Mining.Concepts.and.Techniques.2nd.Ed-1558609016.pdf"><em>Data mining: concepts and techniques</em></a>. → <strong>ch.2.5, ch.10.4.2</strong><br/>» 2017. VanderPlas, Jake; <a href="https://github.com/jakevdp/PythonDataScienceHandbook"><em>Python Data Science Handbook</em></a>. → <strong>ch.5.10 (In-depth Principal Components Analysis notebook)</strong><br/>» (website) -- 2017; <em>sklearn.neighbors.DistanceMetric class</em>: <a href="http://scikit-learn.org/stable/modules/generated/sklearn.neighbors.DistanceMetric.html" class="uri">http://scikit-learn.org/stable/modules/generated/sklearn.neighbors.DistanceMetric.html</a>. → <strong>euclidean, cosine, jaccard</strong><br/><br/><strong>OPTIONAL</strong><br/>› 1997. Charles M. Grinstead, CM and Snell, JL; <a href="http://www.dartmouth.edu/~chance/teaching_aids/books_articles/probability_book/amsbook.mac.pdf"><em>Introduction to Probability</em></a>. → <strong>nice introductory resource to probability</strong><br/>› (website) -- 2017; <em>Distance computations (scipy.spatial.distance)</em>: <a href="https://docs.scipy.org/doc/scipy/reference/spatial.distance.html" class="uri">https://docs.scipy.org/doc/scipy/reference/spatial.distance.html</a>. → <strong>cdist, euclidean, cosine, jaccard</strong><br/>› (O'Reilly Data Show podcast) -- 07-06-2017; <em>A framework for building and evaluating data products</em>: <a href="https://www.oreilly.com/ideas/a-framework-for-building-and-evaluating-data-products" class="uri">https://www.oreilly.com/ideas/a-framework-for-building-and-evaluating-data-products</a>. → <strong>listen to the entire interview</strong><br/></td>
</tr>
<tr class="even">
<td align="right"><strong>Homework</strong></td>
<td align="left">--</td>
</tr>
</tbody>
</table>
<hr />
<h2 id="lecture-4-association-rule-mining-pattern-mining">LECTURE 4: ASSOCIATION RULE MINING, PATTERN MINING</h2>
<table>
<colgroup>
<col width="17%" />
<col width="82%" />
</colgroup>
<thead>
<tr class="header">
<th align="right">Week of 9/20</th>
<th align="left"><a href="https://github.com/kmsaumcis/mcis6273_f17_datamining/tree/master/lecture_notes">Lecture Notes</a></th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td align="right"><strong>Content</strong></td>
<td align="left">introduction to concepts for rule and pattern mining; introdcution to apriori algorithm for frequent patterns; motivating the market basket analysis context for pattern mining; exploring addition contexts</td>
</tr>
<tr class="even">
<td align="right"><strong>Expected<br/>Outcomes</strong></td>
<td align="left">• understand concepts behind frequent patterns<br/>• understand association rule mining, apriori algorithm, FP-growth<br/>• apply and compute basic patterns by hand<br/>• identify the contexts for applying pattern mining<br/></td>
</tr>
<tr class="odd">
<td align="right"><strong>Readings &amp;<br/>Supplemental</strong></td>
<td align="left"><strong>REQUIRED</strong><br/>» 2014. Zaki, Mohammed J and Meira Jr, Wagner; <a href="http://www.dataminingbook.info/pmwiki.php"><em>Data mining and analysis: fundamental concepts and algorithms</em></a>. → <strong>ch.8</strong><br/>» 2011. Han, Jiawei and Pei, Jian and Kamber, Micheline; <a href="https://ia800300.us.archive.org/5/items/DataMiningConceptAndTechniques2ndEdition/Data.Mining.Concepts.and.Techniques.2nd.Ed-1558609016.pdf"><em>Data mining: concepts and techniques</em></a>. → <strong>ch.5</strong><br/><br/><strong>OPTIONAL</strong><br/>› (PartiallyDerivative.com podcast) -- 06-13-2017; <em>The Secret Life Of A Data Scientist</em>: <a href="http://partiallyderivative.com/podcast/2017/06/13/the-secret-life-of-a-data-scientist" class="uri">http://partiallyderivative.com/podcast/2017/06/13/the-secret-life-of-a-data-scientist</a>. → <strong>listen to the entire podcast</strong><br/></td>
</tr>
<tr class="even">
<td align="right"><strong>Homework</strong></td>
<td align="left"><strong>DUE:</strong> Monday, 10/2 - midnight<br/>Please see the Blackboard/<a href="https://github.com/kmsaumcis/mcis6273_f17_datamining/homework">Github repo</a> for what to turn in.</td>
</tr>
</tbody>
</table>
<hr />
<h2 id="lecture-5-unsupervised-techniques-introduction-to-clustering">LECTURE 5: UNSUPERVISED TECHNIQUES / INTRODUCTION TO CLUSTERING</h2>
<table>
<colgroup>
<col width="17%" />
<col width="82%" />
</colgroup>
<thead>
<tr class="header">
<th align="right">Week of 9/27</th>
<th align="left"><a href="https://github.com/kmsaumcis/mcis6273_f17_datamining/tree/master/lecture_notes">Lecture Notes</a></th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td align="right"><strong>Content</strong></td>
<td align="left">introduction to cluster analysis and motivations; introduction to unsupervised clustering algorithms; partitioning (k-means, k-mediods); hierarchical agglomerative methods; model-based (expectation-maximization) neural networks (SOM self-organizing maps); visualing with voronoi diagrams</td>
</tr>
<tr class="even">
<td align="right"><strong>Expected<br/>Outcomes</strong></td>
<td align="left">• exposure to unsupervised clustering methods, k-Means<br/>• introduction to key clustering algorithms<br/>• distinguish between partition and model-based algorithms<br/></td>
</tr>
<tr class="odd">
<td align="right"><strong>Readings &amp;<br/>Supplemental</strong></td>
<td align="left"><strong>REQUIRED</strong><br/>» 2014. Zaki, Mohammed J and Meira Jr, Wagner; <a href="http://www.dataminingbook.info/pmwiki.php"><em>Data mining and analysis: fundamental concepts and algorithms</em></a>. → <strong>ch.13, ch.14, ch.15, ch.17</strong><br/>» 2011. Han, Jiawei and Pei, Jian and Kamber, Micheline; <a href="https://ia800300.us.archive.org/5/items/DataMiningConceptAndTechniques2ndEdition/Data.Mining.Concepts.and.Techniques.2nd.Ed-1558609016.pdf"><em>Data mining: concepts and techniques</em></a>. → <strong>ch.7</strong><br/>» (website) -- 2015; <em>Basic Clustering with k-Means</em>: <a href="https://nbviewer.jupyter.org/github/tmbdev/teaching-mmir/blob/master/30-kmeans.ipynb" class="uri">https://nbviewer.jupyter.org/github/tmbdev/teaching-mmir/blob/master/30-kmeans.ipynb</a>. → <strong>Familiarize yourself with the notebook.</strong><br/><br/><strong>OPTIONAL</strong><br/>› (LinearDigressions.com podcast) -- 04-16-2017; <em>Education Analytics</em>: <a href="http://lineardigressions.com/episodes/2017/4/16/education-analytics" class="uri">http://lineardigressions.com/episodes/2017/4/16/education-analytics</a>. → <strong>listen to the entire podcast</strong><br/>› (website) -- --; <em>Programatically understanding Expectation Maximization</em>: <a href="https://nipunbatra.github.io/blog/2014/em.html" class="uri">https://nipunbatra.github.io/blog/2014/em.html</a>. → <strong>read this practical explanation (with Python code) of the EM algorithm</strong><br/></td>
</tr>
<tr class="even">
<td align="right"><strong>Homework</strong></td>
<td align="left"><strong>DUE:</strong> Monday, 10/23 - midnight<br/>Please see the Blackboard/<a href="https://github.com/kmsaumcis/mcis6273_f17_datamining/homework">Github repo</a> for what to turn in.</td>
</tr>
</tbody>
</table>
<hr />
<h2 id="lecture-6-unsupervised-techniques-more-clustering">LECTURE 6: UNSUPERVISED TECHNIQUES / MORE CLUSTERING</h2>
<table>
<colgroup>
<col width="17%" />
<col width="82%" />
</colgroup>
<thead>
<tr class="header">
<th align="right">Week of 10/4</th>
<th align="left"><a href="https://github.com/kmsaumcis/mcis6273_f17_datamining/tree/master/lecture_notes">Lecture Notes</a></th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td align="right"><strong>Content</strong></td>
<td align="left">continued clustering, hierachical algorithms (agglomorative), introduction to density-based algorithms (DBSCAN)</td>
</tr>
<tr class="even">
<td align="right"><strong>Expected<br/>Outcomes</strong></td>
<td align="left">• understand hierarchical and density-based algorithms<br/>• develop intuitions for choosing algorithms in various contexts<br/>• utilize algorithms on read-world data<br/></td>
</tr>
<tr class="odd">
<td align="right"><strong>Readings &amp;<br/>Supplemental</strong></td>
<td align="left">No assigned readings. Please complete readings from previous week if not current.</td>
</tr>
<tr class="even">
<td align="right"><strong>Homework</strong></td>
<td align="left">--</td>
</tr>
</tbody>
</table>
<hr />
<h2 id="lecture-7-supervised-techniques-classification-and-prediction">LECTURE 7: SUPERVISED TECHNIQUES / CLASSIFICATION AND PREDICTION</h2>
<table>
<colgroup>
<col width="17%" />
<col width="82%" />
</colgroup>
<thead>
<tr class="header">
<th align="right">Week of 10/11</th>
<th align="left"><a href="https://github.com/kmsaumcis/mcis6273_f17_datamining/tree/master/lecture_notes">Lecture Notes</a></th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td align="right"><strong>Content</strong></td>
<td align="left">classification and prediction; understanding decision trees, concepts and theory; probabilistic approaches to classification - naïve bayes; introduction to bayesian belief networks</td>
</tr>
<tr class="even">
<td align="right"><strong>Expected<br/>Outcomes</strong></td>
<td align="left">• understand and explain decision trees<br/>• develop probabilistic models of classification using naïve Bayes<br/>• identify BBNs and their application context<br/>• utilize naïve Bayes in real-world applications<br/></td>
</tr>
<tr class="odd">
<td align="right"><strong>Readings &amp;<br/>Supplemental</strong></td>
<td align="left"><strong>REQUIRED</strong><br/>» 2014. Zaki, Mohammed J and Meira Jr, Wagner; <a href="http://www.dataminingbook.info/pmwiki.php"><em>Data mining and analysis: fundamental concepts and algorithms</em></a>. → <strong>ch.18, ch.19</strong><br/>» 2011. Han, Jiawei and Pei, Jian and Kamber, Micheline; <a href="https://ia800300.us.archive.org/5/items/DataMiningConceptAndTechniques2ndEdition/Data.Mining.Concepts.and.Techniques.2nd.Ed-1558609016.pdf"><em>Data mining: concepts and techniques</em></a>. → <strong>ch.6.3, ch.6.4</strong><br/><br/><strong>OPTIONAL</strong><br/>› (DataSkeptic.com podcast) -- 08-04-2017; <em>MINI: Bayesian Belief Networks</em>: <a href="https://dataskeptic.com/blog/episodes/2017/bayesian-belief-networks" class="uri">https://dataskeptic.com/blog/episodes/2017/bayesian-belief-networks</a>. → <strong>explore this light discussion of BBNs</strong><br/>› 2012. Barber, D.; <a href="http://web4.cs.ucl.ac.uk/staff/D.Barber/textbook/020217.pdf"><em>Bayesian Reasoning and Machine Learning</em></a>. → <strong>explore ch.3 for in a deeper theoretical treatment of BBNs</strong><br/></td>
</tr>
<tr class="even">
<td align="right"><strong>Homework</strong></td>
<td align="left"><strong>DUE:</strong> Monday, 11/3 - midnight<br/>Please see the Blackboard/<a href="https://github.com/kmsaumcis/mcis6273_f17_datamining/homework">Github repo</a> for what to turn in.</td>
</tr>
</tbody>
</table>
<hr />
<h2 id="lecture-8-supervised-techniques-classification-and-prediction">LECTURE 8: SUPERVISED TECHNIQUES / CLASSIFICATION AND PREDICTION</h2>
<table>
<colgroup>
<col width="17%" />
<col width="82%" />
</colgroup>
<thead>
<tr class="header">
<th align="right">Week of 10/18</th>
<th align="left"><a href="https://github.com/kmsaumcis/mcis6273_f17_datamining/tree/master/lecture_notes">Lecture Notes</a></th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td align="right"><strong>Content</strong></td>
<td align="left">linear regression models for prediction; logistic regression models for prediction; introduction to generalized linear models</td>
</tr>
<tr class="even">
<td align="right"><strong>Expected<br/>Outcomes</strong></td>
<td align="left">• understand and develop linear regression models<br/>• understand and interpret logistic regression models<br/>• exposure to generalized linear models<br/></td>
</tr>
<tr class="odd">
<td align="right"><strong>Readings &amp;<br/>Supplemental</strong></td>
<td align="left"><strong>REQUIRED</strong><br/>» 2014. Zaki, Mohammed J and Meira Jr, Wagner; <a href="http://www.dataminingbook.info/pmwiki.php"><em>Data mining and analysis: fundamental concepts and algorithms</em></a>. → <strong>ch.20</strong><br/>» 2011. Han, Jiawei and Pei, Jian and Kamber, Micheline; <a href="https://ia800300.us.archive.org/5/items/DataMiningConceptAndTechniques2ndEdition/Data.Mining.Concepts.and.Techniques.2nd.Ed-1558609016.pdf"><em>Data mining: concepts and techniques</em></a>. → <strong>ch.6.11</strong><br/><br/><strong>OPTIONAL</strong><br/>› (DataSkeptic.com podcast) -- 01-27-2017; <em>MINI: Logistic Regression on Audio Data</em>: <a href="https://dataskeptic.com/blog/episodes/2017/logistic-regression-on-audio-data" class="uri">https://dataskeptic.com/blog/episodes/2017/logistic-regression-on-audio-data</a>. → <strong>listen to the entire podcast</strong><br/>› (website) -- --; <em>Building a logistic regression classifier from the ground up</em>: <a href="http://inmachineswetrust.com/posts/building-logistic-regression/" class="uri">http://inmachineswetrust.com/posts/building-logistic-regression/</a>. → <strong>this is a nice explanation (and code) in Python</strong><br/></td>
</tr>
<tr class="even">
<td align="right"><strong>Homework</strong></td>
<td align="left">--</td>
</tr>
</tbody>
</table>
<hr />
<h2 id="lecture-9-supervised-techniques-classification-and-model-evaluation">LECTURE 9: SUPERVISED TECHNIQUES / CLASSIFICATION AND MODEL EVALUATION</h2>
<table>
<colgroup>
<col width="17%" />
<col width="82%" />
</colgroup>
<thead>
<tr class="header">
<th align="right">Week of 10/25</th>
<th align="left"><a href="https://github.com/kmsaumcis/mcis6273_f17_datamining/tree/master/lecture_notes">Lecture Notes</a></th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td align="right"><strong>Content</strong></td>
<td align="left">support vector machines; neural networks and the basic NN model and its relation to learning algorithms; evaluating models and applying techniques to model validation</td>
</tr>
<tr class="even">
<td align="right"><strong>Expected<br/>Outcomes</strong></td>
<td align="left">• understand support vector machines and their strengths<br/>• understand neural networks, their basic theory and application<br/>• identify and develop intutition around model evaluation and validation<br/></td>
</tr>
<tr class="odd">
<td align="right"><strong>Readings &amp;<br/>Supplemental</strong></td>
<td align="left"><strong>REQUIRED</strong><br/>» 2014. Zaki, Mohammed J and Meira Jr, Wagner; <a href="http://www.dataminingbook.info/pmwiki.php"><em>Data mining and analysis: fundamental concepts and algorithms</em></a>. → <strong>ch.21</strong><br/>» 2011. Han, Jiawei and Pei, Jian and Kamber, Micheline; <a href="https://ia800300.us.archive.org/5/items/DataMiningConceptAndTechniques2ndEdition/Data.Mining.Concepts.and.Techniques.2nd.Ed-1558609016.pdf"><em>Data mining: concepts and techniques</em></a>. → <strong>ch.6.6, ch.6.7</strong><br/><br/><strong>OPTIONAL</strong><br/>› (DataSkeptic.com podcast) -- 05-27-2017; <em>Data Science at eHarmony</em>: <a href="https://dataskeptic.com/blog/episodes/2016/data-science-at-eharmony" class="uri">https://dataskeptic.com/blog/episodes/2016/data-science-at-eharmony</a>. → <strong>listen to the entire podcast</strong><br/></td>
</tr>
<tr class="even">
<td align="right"><strong>Homework</strong></td>
<td align="left"><strong>DUE:</strong> Monday, 11/30 - midnight<br/>Please see the Blackboard/<a href="https://github.com/kmsaumcis/mcis6273_f17_datamining/homework">Github repo</a> for what to turn in.</td>
</tr>
</tbody>
</table>
<hr />
<h2 id="lecture-10-ensemble-methods">LECTURE 10: ENSEMBLE METHODS</h2>
<table>
<colgroup>
<col width="17%" />
<col width="82%" />
</colgroup>
<thead>
<tr class="header">
<th align="right">Week of 11/1</th>
<th align="left"><a href="https://github.com/kmsaumcis/mcis6273_f17_datamining/tree/master/lecture_notes">Lecture Notes</a></th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td align="right"><strong>Content</strong></td>
<td align="left">ensemble methods; introduction to boosting, bagging, random forests and related methods</td>
</tr>
<tr class="even">
<td align="right"><strong>Expected<br/>Outcomes</strong></td>
<td align="left">• understand and identify the need for ensembles<br/>• identify and develop intutition around ensemble model evaluation and validation<br/></td>
</tr>
<tr class="odd">
<td align="right"><strong>Readings &amp;<br/>Supplemental</strong></td>
<td align="left"><strong>REQUIRED</strong><br/>» 2014. Zaki, Mohammed J and Meira Jr, Wagner; <a href="http://www.dataminingbook.info/pmwiki.php"><em>Data mining and analysis: fundamental concepts and algorithms</em></a>. → <strong>ch.22</strong><br/>» 2011. Han, Jiawei and Pei, Jian and Kamber, Micheline; <a href="https://ia800300.us.archive.org/5/items/DataMiningConceptAndTechniques2ndEdition/Data.Mining.Concepts.and.Techniques.2nd.Ed-1558609016.pdf"><em>Data mining: concepts and techniques</em></a>. → <strong>ch.6.12, ch.6.13, ch.6.14, ch.6.15</strong><br/></td>
</tr>
<tr class="even">
<td align="right"><strong>Homework</strong></td>
<td align="left">--</td>
</tr>
</tbody>
</table>
<hr />
<h2 id="lecture-11-data-visualization-introductory-concepts">LECTURE 11: DATA VISUALIZATION: INTRODUCTORY CONCEPTS</h2>
<table>
<colgroup>
<col width="17%" />
<col width="82%" />
</colgroup>
<thead>
<tr class="header">
<th align="right">Week of 11/8</th>
<th align="left"><a href="https://github.com/kmsaumcis/mcis6273_f17_datamining/tree/master/lecture_notes">Lecture Notes</a></th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td align="right"><strong>Content</strong></td>
<td align="left">introduction to data visualization; building data narratives</td>
</tr>
<tr class="even">
<td align="right"><strong>Expected<br/>Outcomes</strong></td>
<td align="left">• understand core social mining algorithms<br/>• understand concepts in network analysis<br/></td>
</tr>
<tr class="odd">
<td align="right"><strong>Readings &amp;<br/>Supplemental</strong></td>
<td align="left"><strong>REQUIRED</strong><br/>» 2015. Knaflic, Cole Nussbaumer; <a href="http://www.storytellingwithdata.com/book/"><em>Storytelling with data: A data visualization guide for business professionals</em></a>. → <strong>ch.8</strong><br/>» (website) -- 2017; <em>D3.js: Data-Driven Documents</em>: <a href="http://d3js.org" class="uri">http://d3js.org</a>. → <strong>familiarize yourself with some of the visualizations and capabilities of D3.js</strong><br/><br/><strong>OPTIONAL</strong><br/>› 2014. B\&quot;orner, Katy and Polley, David E; <a href="https://mitpress.mit.edu/books/visual-insights"><em>Visual insights: A practical guide to making sense of data</em></a>. → <strong>ch.5</strong><br/>› (website) -- 2017; <em>Analyzing Scrabble Games</em>: <a href="http://rpubs.com/jalapic/scrabblr" class="uri">http://rpubs.com/jalapic/scrabblr</a>. → <strong>This is a very interesting exploration in analysis and visualization.</strong><br/>› (website) -- 2017; <em>World Population Growth</em>: <a href="https://ourworldindata.org/world-population-growth/" class="uri">https://ourworldindata.org/world-population-growth/</a>. → <strong>explore some of the data and visualizations</strong><br/>› (website) -- 2017; <em>RAWGraphs: The missing link between spreadsheets and data visualization</em>: <a href="http://rawgraphs.io/" class="uri">http://rawgraphs.io/</a>. → <strong>explore this site and its galleries</strong><br/>› (website) -- 2016; <em>Rio 2016 Medals Race: An analysis of the 2016 Olympic Medals</em>: <a href="http://timesofoman.com/extra/rio_2016_medal_tally/index.html" class="uri">http://timesofoman.com/extra/rio_2016_medal_tally/index.html</a>. → <strong>explore this visualization</strong><br/></td>
</tr>
<tr class="even">
<td align="right"><strong>Homework</strong></td>
<td align="left">--</td>
</tr>
</tbody>
</table>
<hr />
<h2 id="lecture-12-introduction-to-social-mining">LECTURE 12: INTRODUCTION TO SOCIAL MINING</h2>
<table>
<colgroup>
<col width="17%" />
<col width="82%" />
</colgroup>
<thead>
<tr class="header">
<th align="right">Week of 11/15</th>
<th align="left"><a href="https://github.com/kmsaumcis/mcis6273_f17_datamining/tree/master/lecture_notes">Lecture Notes</a></th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td align="right"><strong>Content</strong></td>
<td align="left">introduction to social mining; introduction to recommendation systems, collaborative and content-based filtering</td>
</tr>
<tr class="even">
<td align="right"><strong>Expected<br/>Outcomes</strong></td>
<td align="left">• understand core social mining algorithms<br/>• understand concepts in network analysis<br/>• understand core recommender system concepts<br/></td>
</tr>
<tr class="odd">
<td align="right"><strong>Readings &amp;<br/>Supplemental</strong></td>
<td align="left"><strong>REQUIRED</strong><br/>» 2014. Leskovec, Jure and Rajaraman, Anand and Ullman, Jeffrey David; <a href="http://www.mmds.org/"><em>Mining of massive datasets</em></a>. → <strong>ch.10</strong><br/>» 2015. Grus, Joel; <a href="http://shop.oreilly.com/product/0636920033400.do"><em>Data science from scratch: First principles with Python</em></a>. → <strong>ch.22</strong><br/><br/><strong>OPTIONAL</strong><br/>› 2014. Leskovec, Jure and Rajaraman, Anand and Ullman, Jeffrey David; <a href="http://www.mmds.org/"><em>Mining of massive datasets</em></a>. → <strong>ch.9</strong><br/>› 2014. B\&quot;orner, Katy and Polley, David E; <a href="https://mitpress.mit.edu/books/visual-insights"><em>Visual insights: A practical guide to making sense of data</em></a>. → <strong>ch.5</strong><br/></td>
</tr>
<tr class="even">
<td align="right"><strong>Homework</strong></td>
<td align="left">--</td>
</tr>
</tbody>
</table>
<hr />
<h2 id="lecture-13-introduction-to-text-mining">LECTURE 13: INTRODUCTION TO TEXT MINING</h2>
<table>
<colgroup>
<col width="17%" />
<col width="82%" />
</colgroup>
<thead>
<tr class="header">
<th align="right">Week of 11/29</th>
<th align="left"><a href="https://github.com/kmsaumcis/mcis6273_f17_datamining/tree/master/lecture_notes">Lecture Notes</a></th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td align="right"><strong>Content</strong></td>
<td align="left">introduction to text mining; concepts in document preparation pipeline (tokenizing, stemming, etc.); TFIDF, cosine similarity; corpus selection</td>
</tr>
<tr class="even">
<td align="right"><strong>Expected<br/>Outcomes</strong></td>
<td align="left">• understand introductory concepts in text mining and information retrieval<br/>• understand document preparation tools<br/>• apply basic concepts to real-world data<br/></td>
</tr>
<tr class="odd">
<td align="right"><strong>Readings &amp;<br/>Supplemental</strong></td>
<td align="left"><strong>REQUIRED</strong><br/>» 2011. Han, Jiawei and Pei, Jian and Kamber, Micheline; <a href="https://ia800300.us.archive.org/5/items/DataMiningConceptAndTechniques2ndEdition/Data.Mining.Concepts.and.Techniques.2nd.Ed-1558609016.pdf"><em>Data mining: concepts and techniques</em></a>. → <strong>ch.10.4</strong><br/>» 2008. Manning, Christopher D and Raghavan, Prabhakar and Sch\&quot;utze, Hinrich; <a href="https://nlp.stanford.edu/IR-book/information-retrieval-book.html"><em>Introduction to information retrieval</em></a>. → <strong>ch.6</strong><br/><br/><strong>OPTIONAL</strong><br/>› 2008. Manning, Christopher D and Raghavan, Prabhakar and Sch\&quot;utze, Hinrich; <a href="https://nlp.stanford.edu/IR-book/information-retrieval-book.html"><em>Introduction to information retrieval</em></a>. → <strong>ch.13</strong><br/>› (O'Reilly Data Show podcast) -- 07-06-2017; <em>Language understanding remains one of AI’s grand challenges</em>: <a href="https://www.oreilly.com/ideas/language-understanding-remains-one-of-ais-grand-challenges" class="uri">https://www.oreilly.com/ideas/language-understanding-remains-one-of-ais-grand-challenges</a>. → <strong>listen to the entire interview</strong><br/>› (LinearDigressions.com podcast) -- 04-30-2017; <em>Word2Vec</em>: <a href="http://lineardigressions.com/episodes/2017/4/30/word2vec" class="uri">http://lineardigressions.com/episodes/2017/4/30/word2vec</a>. → <strong>listen to the entire podcast</strong><br/></td>
</tr>
<tr class="even">
<td align="right"><strong>Homework</strong></td>
<td align="left">--</td>
</tr>
</tbody>
</table>
<hr />
<h2 id="lecture-14-open-data-ethics-in-data-mining-the-future-of-data-science">LECTURE 14: OPEN DATA, ETHICS IN DATA MINING, THE FUTURE OF DATA SCIENCE</h2>
<table>
<colgroup>
<col width="17%" />
<col width="82%" />
</colgroup>
<thead>
<tr class="header">
<th align="right">Week of 12/6</th>
<th align="left"><a href="https://github.com/kmsaumcis/mcis6273_f17_datamining/tree/master/lecture_notes">Lecture Notes</a></th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td align="right"><strong>Content</strong></td>
<td align="left">open data portals, APIs, tools and technologies; ethics in data mining; anonymization, privacy and data considerations; data science and the future</td>
</tr>
<tr class="even">
<td align="right"><strong>Expected<br/>Outcomes</strong></td>
<td align="left">• exposure to open data portals and open data technologies<br/>• exposure to open APIs and tools for open data access<br/>• understand data mining ethics and why ethics (and privacy) are critically important<br/>• the future to data science, analytics and intelligent systems built on big data<br/></td>
</tr>
<tr class="odd">
<td align="right"><strong>Readings &amp;<br/>Supplemental</strong></td>
<td align="left"><strong>REQUIRED</strong><br/>» (DataStori.es podcast) -- 05-18-2016; <em>74 - Data Ethics and Privacy with Eleanor Saitta</em>: <a href="http://datastori.es/74-data-ethics-and-privacy-with-eleanor-saitta/" class="uri">http://datastori.es/74-data-ethics-and-privacy-with-eleanor-saitta/</a>. → <strong>listen to the entire podcast</strong><br/>» (website) -- 2017; <em>ProgrammableWeb.com: The Journal of the API Economy</em>: <a href="https://www.programmableweb.com/" class="uri">https://www.programmableweb.com/</a>. → <strong>familiarize yourself with this site and some APIs</strong><br/><br/><strong>OPTIONAL</strong><br/>› (LinearDigressions.com podcast) -- 08-13-2017; <em>Curing Cancer with Machine Learning is Super Hard</em>: <a href="http://lineardigressions.com/episodes/2017/8/13/curing-cancer-with-machine-learning-is-super-hard" class="uri">http://lineardigressions.com/episodes/2017/8/13/curing-cancer-with-machine-learning-is-super-hard</a>. → <strong>listen to the entire podcast</strong><br/></td>
</tr>
<tr class="even">
<td align="right"><strong>Homework</strong></td>
<td align="left">--</td>
</tr>
</tbody>
</table>
<hr />
<h2 id="resources">RESOURCES</h2>
<ol style="list-style-type: decimal">
<li>2014. Zaki, Mohammed J and Meira Jr, Wagner; <a href="http://www.dataminingbook.info/pmwiki.php"><em>Data mining and analysis: fundamental concepts and algorithms</em></a>.<br/></li>
<li>2014. Leskovec, Jure and Rajaraman, Anand and Ullman, Jeffrey David; <a href="http://www.mmds.org/"><em>Mining of massive datasets</em></a>.<br/></li>
<li>1997. Charles M. Grinstead, CM and Snell, JL; <a href="http://www.dartmouth.edu/~chance/teaching_aids/books_articles/probability_book/amsbook.mac.pdf"><em>Introduction to Probability</em></a>.<br/></li>
<li>2011. Yau, Nathan; <a href="http://book.flowingdata.com/"><em>Visualize this: the FlowingData guide to design, visualization, and statistics</em></a>.<br/></li>
<li>2014. B{\&quot;o}rner, Katy and Polley, David E; <a href="https://mitpress.mit.edu/books/visual-insights"><em>Visual insights: A practical guide to making sense of data</em></a>.<br/></li>
<li>2012. Downey, Allen; <a href="http://www.greenteapress.com/thinkpython/thinkpython.html"><em>Think Python</em></a>.<br/></li>
<li>2012. Conway, Drew and White, John; <a href="http://shop.oreilly.com/product/0636920018483.do"><em>Machine learning for hackers</em></a>.<br/></li>
<li>2015. Grus, Joel; <a href="http://shop.oreilly.com/product/0636920033400.do"><em>Data science from scratch: First principles with Python</em></a>.<br/></li>
<li>(website) -- 2017; <em>The Periodic Table of Data Science</em>: <a href="https://www.datacamp.com/community/blog/data-science-periodic-table#gs.TF297Gsm" class="uri">https://www.datacamp.com/community/blog/data-science-periodic-table#gs.TF297Gsm</a>.<br/></li>
<li>2011. Han, Jiawei and Pei, Jian and Kamber, Micheline; <a href="https://ia800300.us.archive.org/5/items/DataMiningConceptAndTechniques2ndEdition/Data.Mining.Concepts.and.Techniques.2nd.Ed-1558609016.pdf"><em>Data mining: concepts and techniques</em></a>.<br/></li>
<li>2012. McKinney, Wes; <a href="https://github.com/wesm/pydata-book"><em>Python for data analysis: Data wrangling with Pandas, NumPy, and IPython</em></a>.<br/></li>
<li>2008. Manning, Christopher D and Raghavan, Prabhakar and Sch{\&quot;u}tze, Hinrich; <a href="https://nlp.stanford.edu/IR-book/information-retrieval-book.html"><em>Introduction to information retrieval</em></a>.<br/></li>
<li>2015. Knaflic, Cole Nussbaumer; <a href="http://www.storytellingwithdata.com/book/"><em>Storytelling with data: A data visualization guide for business professionals</em></a>.<br/></li>
<li>2016. Rose, Doug; <a href="http://www.apress.com/us/book/9781484222522"><em>Data Science: Create Teams That Ask the Right Questions and Deliver Real Value</em></a>.<br/></li>
<li>(website) -- 2013; <em>Mining the Social Web: Data Mining Facebook, Twitter, LinkedIn, Google+, GitHub, and More</em>: <a href="https://github.com/ptwobrussell/Mining-the-Social-Web-2nd-Edition/" class="uri">https://github.com/ptwobrussell/Mining-the-Social-Web-2nd-Edition/</a>.<br/></li>
<li>2017. Wexler, Steve and Shaffer, Jeffrey and Cotgreave, Andy; <a href="http://bigbookofdashboards.com"><em>The Big Book of Dashboards: Visualizing Your Data Using Real-World Business Scenarios</em></a>.<br/></li>
<li>2017. VanderPlas, Jake; <a href="https://github.com/jakevdp/PythonDataScienceHandbook"><em>Python Data Science Handbook</em></a>.<br/></li>
<li>(website) -- 2015; <em>Basic Clustering with k-Means</em>: <a href="https://nbviewer.jupyter.org/github/tmbdev/teaching-mmir/blob/master/30-kmeans.ipynb" class="uri">https://nbviewer.jupyter.org/github/tmbdev/teaching-mmir/blob/master/30-kmeans.ipynb</a>.<br/></li>
<li>(website) -- 2017; <em>Distance computations (scipy.spatial.distance)</em>: <a href="https://docs.scipy.org/doc/scipy/reference/spatial.distance.html" class="uri">https://docs.scipy.org/doc/scipy/reference/spatial.distance.html</a>.<br/></li>
<li>(website) -- 11-15-2016; <em>Jupyter Notebook Tutorial: The Definitive Guide</em>: <a href="https://www.datacamp.com/community/tutorials/tutorial-jupyter-notebook#gs.zExWvMw" class="uri">https://www.datacamp.com/community/tutorials/tutorial-jupyter-notebook#gs.zExWvMw</a>.<br/></li>
<li>(website) -- 2017; <em>Pandas Cookbook</em>: <a href="https://github.com/jvns/pandas-cookbook" class="uri">https://github.com/jvns/pandas-cookbook</a>.<br/></li>
<li>(website) -- 2017; <em>sklearn.neighbors.DistanceMetric class</em>: <a href="http://scikit-learn.org/stable/modules/generated/sklearn.neighbors.DistanceMetric.html" class="uri">http://scikit-learn.org/stable/modules/generated/sklearn.neighbors.DistanceMetric.html</a>.<br/></li>
<li>({Michael Kennedy's Talk Python To Me} podcast) -- 11-28-2016; <em>Episode #90: Data Wrangling with Python</em>: <a href="http://talkpythontome.fm" class="uri">http://talkpythontome.fm</a>.<br/></li>
<li>({O'Reilly Data Show} podcast) -- 07-06-2017; <em>A framework for building and evaluating data products</em>: <a href="https://www.oreilly.com/ideas/a-framework-for-building-and-evaluating-data-products" class="uri">https://www.oreilly.com/ideas/a-framework-for-building-and-evaluating-data-products</a>.<br/></li>
<li>({O'Reilly Data Show} podcast) -- 07-06-2017; <em>Language understanding remains one of AI’s grand challenges</em>: <a href="https://www.oreilly.com/ideas/language-understanding-remains-one-of-ais-grand-challenges" class="uri">https://www.oreilly.com/ideas/language-understanding-remains-one-of-ais-grand-challenges</a>.<br/></li>
<li>(PartiallyDerivative.com podcast) -- 06-13-2017; <em>The Secret Life Of A Data Scientist</em>: <a href="http://partiallyderivative.com/podcast/2017/06/13/the-secret-life-of-a-data-scientist" class="uri">http://partiallyderivative.com/podcast/2017/06/13/the-secret-life-of-a-data-scientist</a>.<br/></li>
<li>(LinearDigressions.com podcast) -- 04-16-2017; <em>Education Analytics</em>: <a href="http://lineardigressions.com/episodes/2017/4/16/education-analytics" class="uri">http://lineardigressions.com/episodes/2017/4/16/education-analytics</a>.<br/></li>
<li>(LinearDigressions.com podcast) -- 06-04-2017; <em>PageRank</em>: <a href="http://lineardigressions.com/episodes/2017/6/4/pagerank" class="uri">http://lineardigressions.com/episodes/2017/6/4/pagerank</a>.<br/></li>
<li>(LinearDigressions.com podcast) -- 08-13-2017; <em>Curing Cancer with Machine Learning is Super Hard</em>: <a href="http://lineardigressions.com/episodes/2017/8/13/curing-cancer-with-machine-learning-is-super-hard" class="uri">http://lineardigressions.com/episodes/2017/8/13/curing-cancer-with-machine-learning-is-super-hard</a>.<br/></li>
<li>(LinearDigressions.com podcast) -- 04-30-2017; <em>Word2Vec</em>: <a href="http://lineardigressions.com/episodes/2017/4/30/word2vec" class="uri">http://lineardigressions.com/episodes/2017/4/30/word2vec</a>.<br/></li>
<li>(DataStori.es podcast) -- 05-18-2016; <em>74 - Data Ethics and Privacy with Eleanor Saitta</em>: <a href="http://datastori.es/74-data-ethics-and-privacy-with-eleanor-saitta/" class="uri">http://datastori.es/74-data-ethics-and-privacy-with-eleanor-saitta/</a>.<br/></li>
<li>(website) -- 2017; <em>ProgrammableWeb.com: The Journal of the API Economy</em>: <a href="https://www.programmableweb.com/" class="uri">https://www.programmableweb.com/</a>.<br/></li>
<li>(website) -- 2017; <em>Analyzing Scrabble Games</em>: <a href="http://rpubs.com/jalapic/scrabblr" class="uri">http://rpubs.com/jalapic/scrabblr</a>.<br/></li>
<li>(website) -- 2017; <em>GSS Data Explorer</em>: <a href="https://gssdataexplorer.norc.org/" class="uri">https://gssdataexplorer.norc.org/</a>.<br/></li>
<li>(website) -- 2017; <em>World Population Growth</em>: <a href="https://ourworldindata.org/world-population-growth/" class="uri">https://ourworldindata.org/world-population-growth/</a>.<br/></li>
<li>(website) -- 2017; <em>RAWGraphs: The missing link between spreadsheets and data visualization</em>: <a href="http://rawgraphs.io/" class="uri">http://rawgraphs.io/</a>.<br/></li>
<li>(website) -- 2016; <em>Rio 2016 Medals Race: An analysis of the 2016 Olympic Medals</em>: <a href="http://timesofoman.com/extra/rio_2016_medal_tally/index.html" class="uri">http://timesofoman.com/extra/rio_2016_medal_tally/index.html</a>.<br/></li>
<li>(website) -- 2017; <em>D3.js: Data-Driven Documents</em>: <a href="http://d3js.org" class="uri">http://d3js.org</a>.<br/></li>
<li>(DataSkeptic.com podcast) -- 08-04-2017; <em>MINI: Bayesian Belief Networks</em>: <a href="https://dataskeptic.com/blog/episodes/2017/bayesian-belief-networks" class="uri">https://dataskeptic.com/blog/episodes/2017/bayesian-belief-networks</a>.<br/></li>
<li>(DataSkeptic.com podcast) -- 01-27-2017; <em>MINI: Logistic Regression on Audio Data</em>: <a href="https://dataskeptic.com/blog/episodes/2017/logistic-regression-on-audio-data" class="uri">https://dataskeptic.com/blog/episodes/2017/logistic-regression-on-audio-data</a>.<br/></li>
<li>(DataSkeptic.com podcast) -- 05-27-2017; <em>Data Science at eHarmony</em>: <a href="https://dataskeptic.com/blog/episodes/2016/data-science-at-eharmony" class="uri">https://dataskeptic.com/blog/episodes/2016/data-science-at-eharmony</a>.<br/></li>
<li>(website) -- --; <em>Programatically understanding Expectation Maximization</em>: <a href="https://nipunbatra.github.io/blog/2014/em.html" class="uri">https://nipunbatra.github.io/blog/2014/em.html</a>.<br/></li>
<li>(website) -- --; <em>Building a logistic regression classifier from the ground up</em>: <a href="http://inmachineswetrust.com/posts/building-logistic-regression/" class="uri">http://inmachineswetrust.com/posts/building-logistic-regression/</a>.<br/></li>
<li>2012. Barber, D.; <a href="http://web4.cs.ucl.ac.uk/staff/D.Barber/textbook/020217.pdf"><em>{Bayesian Reasoning and Machine Learning}</em></a>.<br/></li>
</ol>
