<!DOCTYPE html>
<html lang="en-us">
  <head>
    <meta charset="utf-8">
    <meta name="viewport" content="width=device-width, initial-scale=1">
    <title> | MCIS6273 | Data Mining | Fall 2018</title>
    <link rel="stylesheet" href="/mcis6273_f18_datamining/css/style.css" />
    <link rel="stylesheet" href="/mcis6273_f18_datamining/css/fonts.css" />
    <style>
@import url('https://fonts.googleapis.com/css?family=Fanwood+Text:400,400i|Faustina:400,400i,600,600i|Libre+Baskerville|Lora:400,400i,700,700i|Merriweather:400,400i,700,700i|Old+Standard+TT|Open+Sans:400,400i,700,700i|Playfair+Display|Radley|Spectral:400,400i,600,600i,700,700i');
</style>

  </head>

  <body>
    <nav>
    <ul class="menu">
      
      <li><a href="/mcis6273_f18_datamining">Home</a></li>
      
      <li><a href="/mcis6273_f18_datamining/syllabus/">Syllabus</a></li>
      
      <li><a href="/mcis6273_f18_datamining/homework/">Homework</a></li>
      
      <li><a href="/mcis6273_f18_datamining/index.xml">Subscribe</a></li>
      
    </ul>
    <hr/>
    </nav>

<div class="article-meta">
<h1><span class="title"></span></h1>


</div>

<main>


<h1 id="mcis6273-data-mining-prof-maull-fall-2018-hw2">MCIS6273 Data Mining (Prof. Maull) / Fall 2018 / HW2</h1>

<p><strong>This assignment is worth up to 40 POINTS to your grade total if you complete it on time.</strong></p>

<table>
<thead>
<tr>
<th align="center">Points <br/>Possible</th>
<th align="center">Due Date</th>
<th align="center">Time Commitment <br/>(estimated)</th>
</tr>
</thead>

<tbody>
<tr>
<td align="center">40</td>
<td align="center">Wednesday, December 05 @ Midnight</td>
<td align="center"><em>up to</em> 6 hours</td>
</tr>
</tbody>
</table>

<ul>
<li><p><strong>GRADING:</strong> Grading will be aligned with the completeness of the objectives.</p></li>

<li><p><strong>INDEPENDENT WORK:</strong> Copying, cheating, plagiarism  and academic dishonesty <em>are not tolerated</em> by Univerisity or course policy.  Please see the syllabus for the full departmental and University statement on the academic code of honor.</p></li>
</ul>

<h2 id="objectives">OBJECTIVES</h2>

<ul>
<li><p>improve on your homework 1 assignment, if necessary</p></li>

<li><p>perform a clustering analysis using k-means</p></li>

<li><p>undestand the issues of data science at scale by listening to a current podcast about data mining and machine learning</p></li>
</ul>

<h2 id="what-to-turn-in">WHAT TO TURN IN</h2>

<p>You are being encouraged to turn the assignment in using the provided
Jupyter Notebook.  To do so, make a directory in your Lab environment called
<code>homework/hw2</code>.   Put all of your files in that directory.  Then zip that directory,
rename it with your name as the first part of the filename (e.g. <code>maull_hw2_files.zip</code>), then
download it to your local machine, then upload the <code>.zip</code> to Blackboard.</p>

<p>If you do not know how to do this, please ask, or visit one of the many tutorials out there
on the basics of using zip in Linux.</p>

<h2 id="assignment-tasks">ASSIGNMENT TASKS</h2>

<h3 id="50-perform-a-clustering-analysis-using-k-means">(50%) perform a clustering analysis using k-means</h3>

<p>We talked about the simplicity and power of k-means algorithm and now we are
going to use it to explore an interesting dataset.</p>

<p>The State of Delaware maintains a dataset of the causes of death from many
different diseases.  We going to explore this dataset in some interesting ways
using unsupervised learning, namely clustering, to do some exploratory
data analysis.</p>

<p><strong>DATA PREPARATION</strong></p>

<ul>
<li>You will want to filter the original data down
to just those data points where you have just the data
for <code>STATE OF RESIDENCE == DELAWARE</code>.</li>
</ul>

<p><strong>MAKE SURE TO SHOW ALL YOUR WORK IN THE NOTEBOOK SO YOU CAN RECEIVE PARTIAL CREDIT WHERE APPROPRIATE!</strong></p>

<p>&#167;  Load the CSV of the data from the URL:</p>

<ul>
<li><a href="https://data.delaware.gov/api/views/nck5-dhqv/rows.csv?accessType=DOWNLOAD"><code>https://data.delaware.gov/api/views/nck5-dhqv/rows.csv?accessType=DOWNLOAD</code></a></li>
</ul>

<p>Please provide a <strong>bar plot</strong> of all the diseases and their frequencies (over all years).
You will need to use the <code>CAUSE OF DEATH</code> column once the data is loaded.  Recall,
you can simply do the loading by <code>pandas.read_csv=(&quot;DATA_URL&quot;)</code>.
You are also free to download the CSV file to a local filesystem, just remember
to include it in your ZIP so I can run your code correctly.</p>

<p>You may want to read up on <a href="http://pandas.pydata.org/pandas-docs/stable/generated/pandas.Series.value_counts.html#pandas.Series.value_counts"><code>Series.value_counts()</code></a>
to aggregate the data and <a href="http://pandas.pydata.org/pandas-docs/stable/generated/pandas.Series.plot.html#pandas.Series.plot">`Series.plot(kind=&lsquo;bar&rsquo;</a>.</p>

<p>Turn in the bar plot showing all the diseases and answer:</p>

<ul>
<li><strong>Excluding &ldquo;all other diseases&rdquo;, what are the top 5 diseases?</strong> (you can
use <code>value_counts()[:6]</code> to quickly answer this or you can look at the plot)</li>
<li><strong>What percentage of the top 10 causes of death were from cancers (i.e. <em>neoplasms</em>)?</strong>
Use <code>value_counts[:11].index.str.contains()</code> (see <a href="http://pandas.pydata.org/pandas-docs/stable/generated/pandas.Series.str.contains.html#pandas.Series.str.contains"><code>Series.str.contains()</code></a>) to do this.</li>
</ul>

<p>&#167;  We want to filter the data further to just cancers (over all data, not just
the top 10).  Use <code>df['CAUSE OF DEATH'].str.contains('neoplas')</code> to do this.</p>

<p>Use <code>value_counts()</code> over <code>YEAR</code> to get total deaths per year.</p>

<ul>
<li><strong>Which year had the highest number of deaths?</strong> Do not worrry about the rate of death,
just the raw total.</li>
<li><strong>Which year had the <em>highest rate</em> of cancer deaths?</strong> <strong>HINT</strong>: If you have two Series
objects of the same size <code>S1['column'].value_counts() / S2['column'].value_counts()</code> will
return a Series with all the computations representing the rate we&rsquo;re looking for.
If S1 is the number of cancer deaths and S2 the total number of deaths filtered by year.</li>
</ul>

<p>&#167;  To prepare our data for clustering, we will need to turn all of our <em>categorical</em> variables
into <em>numeric</em> one&rsquo;s.  The easiest way to do this is to use <code>Pandas.get_dummies(your_dataframe)</code>.</p>

<ul>
<li><strong>How many features are now in your dataframe?</strong></li>
</ul>

<p>&#167;  In class we talked about the fact that the $k$ number of clusters needs to be
determined <em>a priori</em> &ndash; that is you will need to know how many clusters beforehand to
run the algorithm.  To find the optimal $k$, we will use a method called the <em>silhouette score</em>.</p>

<p>Adapt the following code to compute the silhouette scores on <em>only</em> the dataset filtered by
cancer deaths.  For this part, we are not interested any of the other causes of death
except cancer.</p>

<pre><code class="language-python">  from sklearn.cluster import KMeans
  from sklearn.metrics import silhouette_score

  Sum_of_squared_distances = []
  K = range(2,15)
  for k in K:
      km = KMeans(n_clusters=k, n_init=20)
      km = km.fit(YOUR_NEOPLASM_DATAFRAME_WITH_DUMMY_VARS)
      Sum_of_squared_distances.append(km.inertia_)

      silh_score = silhouette_score(YOUR_NEOPLASM_DATAFRAME_WITH_DUMMY_VARS, km.labels_)
      print(&quot;k = {} | silhouette_score = {}&quot;.format(k, silh_score))
</code></pre>

<p>The largest score is typically the $k$ you go with.  If $k=2$ is your largest
  score, we will ignore that since 2 clusters is not usually an interesting number of
  clusters when dealing with a large set of data points.</p>

<p><strong>NOTE:</strong> You may drop the columns
  <code>YEAR</code>, <code>COUNT</code>, <code>STATE OF RESIDENCE_DELAWARE</code>
  from your data since these do not have any bearing on our analysis.</p>

<ul>
<li><strong>What is the optimal $k$ according the silhouette score?</strong></li>
<li><strong>Do you see anything else interesting about the scores?</strong></li>
</ul>

<p>&#167;  Now that you have clusters, let&rsquo;s find out which features dominate them.</p>

<ul>
<li><strong>Print a report of the clusters and their top dominant features adapting
the code below.</strong>  Note that the k-means algorithm returns the cluster centers
for each cluster, hence in that center, we will use the dominant features
as the <em>representative features</em> for that cluster.  For the sake of this
exercise, we will use $0.395$ as the threshold of an interesting feature.  Recall
also that all of the feature values are 0 or 1 and hence we have cluster centroid
values that range between 0 and 1.0.</li>
</ul>

<pre><code class="language-python">    optimal_k = THE_OPTIMAL_SILH_K

    km = KMeans(n_clusters=optimal_k, n_init=150)
    km = km.fit(YOUR_NEOPLASM_DATAFRAME_WITH_DUMMY_VARS)

    for i in range(0, optimal_k):
        l = list(zip(YOUR_NEOPLASM_DATAFRAME_WITH_DUMMY_VARS.columns, \
                    km.cluster_centers_[i]))
        l.sort(key=lambda x: x[1], reverse=True)

        print('CLUSTER : {}\n'.format(i))
        for attr, val in l[:15]:
            if val &gt; 0.395: # we are going to stop printing values &lt; 0.395
                print('\t{} : {}\n'.format(attr, val))
</code></pre>

<h3 id="50-undestand-the-issues-of-data-science-at-scale-by-listening-to-a-current-podcast-about-data-mining-and-machine-learning">(50%) undestand the issues of data science at scale by listening to a current podcast about data mining and machine learning</h3>

<p>Throughout the course, we have talk about various algorithms, methods and tools
for doing data mining.  We&rsquo;ve also talked about &ldquo;Data Mining&rdquo; being a part of
the over all field of &ldquo;Data Science&rdquo;</p>

<p>You already know that there is a lot of talk about &ldquo;Data Science&rdquo; being one of the hottest
(if not <em>the hottest</em>) tech jobs out there.  It is certainly one of the [fastest]() growing
jobs categories in tech, spanning many different domains, but the breadth of the
discipline is large and has many use cases and impacts in many different areas.</p>

<p>We are going to listen to a real-word account of Data Science &ldquo;in the wild&rdquo; and how
one company is dealing with the demands of their customers to bring data science (i.e.
data mining, analytics, visualization and machine learning) into their enterprise
product offerings.</p>

<p>&#167;  Listen to the O&rsquo;Reilley Data Show podcast episode
<a href="https://www.oreilly.com/ideas/building-tools-for-enterprise-data-science">&ldquo;Building tools for enterprise data science&rdquo; (Nov. 21, 2018)</a> (you
can also find it on Soundcloud
<a href="https://soundcloud.com/oreilly-radar/building-tools-for-enterprise-data-science">here https://soundcloud.com/oreilly-radar/building-tools-for-enterprise-data-science</a>),
which is an interview with the VP of Data Science and Engineering, Vitaly Gordon of <a href="https://salesforce.com">Salesforce.com</a>.</p>

<ul>
<li>Why did Salesforce decide to build their own internal platform, Einstein, for delivering data science at scale to their customers?</li>
<li>What estimate does Vitaly give for the amount of data that sits in &ldquo;custom objects&rdquo; or &ldquo;custom tables&rdquo; in their customer data?</li>
<li>What problem does TransmogrifAI try to solve?</li>
<li>What kind of data does the platform work with (e.g. structured or unstructured)?</li>
<li>Give an example of the kind of metadata Vitaly mentions is being used to perform feature engineering?</li>
<li>Why is domain expertise so important when dealing with data pipelines (according to Vitaly)?</li>
<li>What three things did Vitaly identify as having a higher impact on model accuracy than the models themselves?</li>
<li>What does the &ldquo;Prediction Builder&rdquo; tool do for customers?</li>
<li>Why was it important to introduce tools used to help customers monitor models?</li>
<li>What does Vitaly predict will be the future role of data scientists as tool automation continues in AI
and machine learning (e.g. use his comparison with software engineering)?</li>
</ul>

</main>

  <footer>
  <script src="//yihui.name/js/math-code.js"></script>
<script async src="//mathjax.rstudio.com/latest/MathJax.js?config=TeX-MML-AM_CHTML"></script>
<script type="text/x-mathjax-config">
MathJax.Hub.Config({
  tex2jax: {
    inlineMath: [['$','$'], ['\\(','\\)']],
    displayMath: [['$$','$$'], ['\[','\]']],
    processEscapes: true,
    processEnvironments: true,
    skipTags: ['script', 'noscript', 'style', 'textarea', 'pre'],
    TeX: { equationNumbers: { autoNumber: "AMS" },
         extensions: ["AMSmath.js", "AMSsymbols.js"] }
  }
});
</script>
<script async src="//yihui.name/js/center-img.js"></script>

  

  </footer>
  </body>
</html>

