{"cells": [{"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": []}, {"cell_type": "markdown", "metadata": {}, "source": ["# MCIS6273 Data Mining (Prof. Maull) / Fall 2018 / HW2\n", "\n", "**This assignment is worth up to 40 POINTS to your grade total if you complete it on time.**\n", "\n", "| Points <br/>Possible | Due Date | Time Commitment <br/>(estimated) |\n", "|:---------------:|:--------:|:---------------:|\n", "| 40 | Wednesday, December 05 @ Midnight | _up to_ 6 hours |\n", "\n", "\n", "* **GRADING:** Grading will be aligned with the completeness of the objectives.\n", "\n", "* **INDEPENDENT WORK:** Copying, cheating, plagiarism  and academic dishonesty _are not tolerated_ by Univerisity or course policy.  Please see the syllabus for the full departmental and University statement on the academic code of honor.\n", "\n", "## OBJECTIVES\n", "* improve on your homework 1 assignment, if necessary\n", "\n", "* perform a clustering analysis using k-means\n", "\n", "* understand the issues of data science at scale by listening to a current podcast about data mining and machine learning\n", "\n", "## WHAT TO TURN IN\n", "You are being encouraged to turn the assignment in using the provided\n", "Jupyter Notebook.  To do so, make a directory in your Lab environment called\n", "`homework/hw2`.   Put all of your files in that directory.  Then zip that directory,\n", "rename it with your name as the first part of the filename (e.g. `maull_hw2_files.zip`), then\n", "download it to your local machine, then upload the `.zip` to Blackboard.\n", "\n", "If you do not know how to do this, please ask, or visit one of the many tutorials out there\n", "on the basics of using zip in Linux.\n", "\n", "## ASSIGNMENT TASKS\n", "### (50%) perform a clustering analysis using k-means \n", "\n", "We talked about the simplicity and power of k-means algorithm and now we are\n", "going to use it to explore an interesting dataset.\n", "\n", "The State of Delaware maintains a dataset of the causes of death from many\n", "different diseases.  We going to explore this dataset in some interesting ways\n", "using unsupervised learning, namely clustering, to do some exploratory\n", "data analysis.\n", "\n", "**DATA PREPARATION**\n", "\n", "* You will want to filter the original data down\n", "to just those data points where you have just the data\n", "for `STATE OF RESIDENCE == DELAWARE`.\n", "\n", "**MAKE SURE TO SHOW ALL YOUR WORK IN THE NOTEBOOK SO YOU CAN RECEIVE PARTIAL CREDIT WHERE APPROPRIATE!**\n", "\n", "&#167;  Load the CSV of the data from the URL:\n", "\n", "* [`https://data.delaware.gov/api/views/nck5-dhqv/rows.csv?accessType=DOWNLOAD`](https://data.delaware.gov/api/views/nck5-dhqv/rows.csv?accessType=DOWNLOAD)\n", "\n", "Please provide a **bar plot** of all the diseases and their frequencies (over all years).\n", "You will need to use the `CAUSE OF DEATH` column once the data is loaded.  Recall,\n", "you can simply do the loading by `pandas.read_csv=(\"DATA_URL\")`.\n", "You are also free to download the CSV file to a local filesystem, just remember\n", "to include it in your ZIP so I can run your code correctly.\n", "\n", "You may want to read up on [`Series.value_counts()`](http://pandas.pydata.org/pandas-docs/stable/generated/pandas.Series.value_counts.html#pandas.Series.value_counts)\n", "to aggregate the data and [`Series.plot(kind='bar'](http://pandas.pydata.org/pandas-docs/stable/generated/pandas.Series.plot.html#pandas.Series.plot).\n", "\n", "Turn in the bar plot showing all the diseases and answer:\n", "\n", "* **Excluding \"all other diseases\", what are the top 5 diseases?** (you can\n", "use `value_counts()[:6]` to quickly answer this or you can look at the plot)\n", "* **What percentage of the top 10 causes of death were from cancers (i.e. _neoplasms_)?**\n", "Use `value_counts[:11].index.str.contains()` (see [`Series.str.contains()`](http://pandas.pydata.org/pandas-docs/stable/generated/pandas.Series.str.contains.html#pandas.Series.str.contains)) to do this.\n", "\n", "\n", "&#167;  We want to filter the data further to just cancers (over all data, not just\n", "the top 10).  Use `df['CAUSE OF DEATH'].str.contains('neoplas')` to do this.\n", "\n", "Use `value_counts()` over `YEAR` to get total deaths per year.\n", "\n", "* **Which year had the highest number of deaths?** Do not worrry about the rate of death,\n", "just the raw total.\n", "* **Which year had the _highest rate_ of cancer deaths?** **HINT**: If you have two Series\n", " objects of the same size `S1['column'].value_counts() / S2['column'].value_counts()` will\n", " return a Series with all the computations representing the rate we're looking for.\n", "  If S1 is the number of cancer deaths and S2 the total number of deaths filtered by year.\n", "\n", "\n", "&#167;  To prepare our data for clustering, we will need to turn all of our _categorical_ variables\n", "into _numeric_ one's.  The easiest way to do this is to use `Pandas.get_dummies(your_dataframe)`.\n", "\n", "* **How many features are now in your dataframe?**\n", "\n", "\n", "&#167;  In class we talked about the fact that the $k$ number of clusters needs to be\n", "determined _a priori_ -- that is you will need to know how many clusters beforehand to\n", "run the algorithm.  To find the optimal $k$, we will use a method called the _silhouette score_.\n", "\n", "Adapt the following code to compute the silhouette scores on *only* the dataset filtered by\n", "cancer deaths.  For this part, we are not interested any of the other causes of death\n", "except cancer.\n", "\n", "```python\n", "  from sklearn.cluster import KMeans\n", "  from sklearn.metrics import silhouette_score\n", "\n", "  Sum_of_squared_distances = []\n", "  K = range(2,15)\n", "  for k in K:\n", "      km = KMeans(n_clusters=k, n_init=20)\n", "      km = km.fit(YOUR_NEOPLASM_DATAFRAME_WITH_DUMMY_VARS)\n", "      Sum_of_squared_distances.append(km.inertia_)\n", "\n", "      silh_score = silhouette_score(YOUR_NEOPLASM_DATAFRAME_WITH_DUMMY_VARS, km.labels_)\n", "      print(\"k = {} | silhouette_score = {}\".format(k, silh_score))\n", "```\n", "\n", "  The largest score is typically the $k$ you go with.  If $k=2$ is your largest\n", "  score, we will ignore that since 2 clusters is not usually an interesting number of\n", "  clusters when dealing with a large set of data points.\n", "\n", "  **NOTE:** You may drop the columns\n", "  `YEAR`, `COUNT`, `STATE OF RESIDENCE_DELAWARE`\n", "  from your data since these do not have any bearing on our analysis.\n", "\n", "  * **What is the optimal $k$ according the silhouette score?**\n", "  * **Do you see anything else interesting about the scores?**\n", "\n", "\n", "&#167;  Now that you have clusters, let's find out which features dominate them.\n", "\n", "* **Print a report of the clusters and their top dominant features adapting\n", "the code below.**  Note that the k-means algorithm returns the cluster centers\n", "for each cluster, hence in that center, we will use the dominant features\n", "as the _representative features_ for that cluster.  For the sake of this\n", "exercise, we will use $0.395$ as the threshold of an interesting feature.  Recall\n", "also that all of the feature values are 0 or 1 and hence we have cluster centroid\n", "values that range between 0 and 1.0.\n", "\n", "```python\n", "    optimal_k = THE_OPTIMAL_SILH_K\n", "\n", "    km = KMeans(n_clusters=optimal_k, n_init=150)\n", "    km = km.fit(YOUR_NEOPLASM_DATAFRAME_WITH_DUMMY_VARS)\n", "\n", "    for i in range(0, optimal_k):\n", "        l = list(zip(YOUR_NEOPLASM_DATAFRAME_WITH_DUMMY_VARS.columns, \\\n", "                    km.cluster_centers_[i]))\n", "        l.sort(key=lambda x: x[1], reverse=True)\n", "\n", "        print('CLUSTER : {}\\n'.format(i))\n", "        for attr, val in l[:15]:\n", "            if val > 0.395: # we are going to stop printing values < 0.395\n", "                print('\\t{} : {}\\n'.format(attr, val))\n", "```\n", "\n", "\n", "\n", "### (50%) understand the issues of data science at scale by listening to a current podcast about data mining and machine learning \n", "\n", "Throughout the course, we have talk about various algorithms, methods and tools\n", "for doing data mining.  We've also talked about \"Data Mining\" being a part of\n", "the over all field of \"Data Science\"\n", "\n", "You already know that there is a lot of talk about \"Data Science\" being one of the hottest\n", "(if not _the hottest_) tech jobs out there.  It is certainly one of the [fastest](https://www.bloomberg.com/news/articles/2018-05-18/-sexiest-job-ignites-talent-wars-as-demand-for-data-geeks-soars)\n", "growing jobs categories in tech, spanning many different domains, but the breadth of the\n", "discipline is large and has many use cases and impacts in many different areas.\n", "\n", "We are going to listen to a real-word account of Data Science \"in the wild\" and how\n", "one company is dealing with the demands of their customers to bring data science (i.e.\n", "data mining, analytics, visualization and machine learning) into their enterprise\n", "product offerings.\n", "\n", "&#167;  Listen to the O'Reilley Data Show podcast episode\n", "[\"Building tools for enterprise data science\" (Nov. 21, 2018)](https://www.oreilly.com/ideas/building-tools-for-enterprise-data-science) (you\n", "can also find it on Soundcloud\n", "[here https://soundcloud.com/oreilly-radar/building-tools-for-enterprise-data-science](https://soundcloud.com/oreilly-radar/building-tools-for-enterprise-data-science)),\n", "which is an interview with the VP of Data Science and Engineering, Vitaly Gordon of [Salesforce.com](https://salesforce.com).\n", "\n", "* Why did Salesforce decide to build their own internal platform, Einstein, for delivering data science at scale to their customers?\n", "* What estimate does Vitaly give for the amount of data that sits in \"custom objects\" or \"custom tables\" in their customer data?\n", "* What problem does TransmogrifAI try to solve?\n", "* What kind of data does the platform work with (e.g. structured or unstructured)?\n", "* Give an example of the kind of metadata Vitaly mentions is being used to perform feature engineering?\n", "* Why is domain expertise so important when dealing with data pipelines (according to Vitaly)?\n", "* What three things did Vitaly identify as having a higher impact on model accuracy than the models themselves?\n", "* What does the \"Prediction Builder\" tool do for customers?\n", "* Why was it important to introduce tools used to help customers monitor models?\n", "* What does Vitaly predict will be the future role of data scientists as tool automation continues in AI\n", "  and machine learning (e.g. use his comparison with software engineering)?\n", "\n", "\n"]}], "metadata": {"anaconda-cloud": {}, "kernelspec": {"display_name": "Python [default]", "language": "python", "name": "python3"}, "language_info": {"codemirror_mode": {"name": "ipython", "version": 3}, "file_extension": ".py", "mimetype": "text/x-python", "name": "python", "nbconvert_exporter": "python", "pygments_lexer": "ipython3", "version": "3.6.1"}, "toc": {"colors": {"hover_highlight": "#DAA520", "navigate_num": "#000000", "navigate_text": "#333333", "running_highlight": "#FF0000", "selected_highlight": "#FFD700", "sidebar_border": "#EEEEEE", "wrapper_background": "#FFFFFF"}, "moveMenuLeft": true, "nav_menu": {"height": "12px", "width": "252px"}, "navigate_menu": true, "number_sections": false, "sideBar": true, "threshold": "1", "toc_cell": false, "toc_section_display": "block", "toc_window_display": true, "widenNotebook": false}}, "nbformat": 4, "nbformat_minor": 0}