<!DOCTYPE html>
<html lang="en-us">
  <head>
    <meta charset="utf-8">
    <meta name="viewport" content="width=device-width, initial-scale=1">
    <title>Syllabus | My New Hugo Site</title>
    <link rel="stylesheet" href="/css/style.css" />
    <link rel="stylesheet" href="/css/fonts.css" />
    <style>
@import url('https://fonts.googleapis.com/css?family=Fanwood+Text:400,400i|Faustina:400,400i,600,600i|Libre+Baskerville|Lora:400,400i,700,700i|Merriweather:400,400i,700,700i|Old+Standard+TT|Open+Sans:400,400i,700,700i|Playfair+Display|Radley|Spectral:400,400i,600,600i,700,700i');
</style>

  </head>

  <body>
    <nav>
    <ul class="menu">
      
      <li><a href="/">Home</a></li>
      
      <li><a href="/syllabus/">Syllabus</a></li>
      
      <li><a href="/homework/">Homework</a></li>
      
      <li><a href="/notes/">Lecture Notes</a></li>
      
      <li><a href="/index.xml">Subscribe</a></li>
      
    </ul>
    <hr/>
    </nav>

<div class="article-meta">
<h1><span class="title">Syllabus</span></h1>


</div>

<main>


<h2 id="lecture-1-class-policies-tools-and-technologies">LECTURE 1: CLASS POLICIES, TOOLS AND TECHNOLOGIES</h2>

<table>
<thead>
<tr>
<th align="right">Week of <sup>8</sup>&frasl;<sub>30</sub></th>
<th align="left"><a href="https://github.com/kmsaumcis/mcis6273_f17_datamining/tree/master/lecture_notes/">Lecture Notes</a></th>
</tr>
</thead>

<tbody>
<tr>
<td align="right"><strong>Content</strong></td>
<td align="left">class policies, class tools, introduction, what this course is about, data mining: tools, technologies and techniques</td>
</tr>

<tr>
<td align="right"><strong>Expected<br/>Outcomes</strong></td>
<td align="left">&#8226; overview of course policies<br/>&#8226; overview of data mining concepts, algorithms, methodologies<br/>&#8226; installation of Anaconda and Python 3.6<br/>&#8226; introduction to Jupyter Notebooks<br/>&#8226; creation of Github account<br/></td>
</tr>

<tr>
<td align="right"><strong>Readings &amp;<br/>Supplemental</strong></td>
<td align="left"><strong>REQUIRED</strong><br/>&raquo; Zaki, Mohammed J and Meira Jr, Wagner;&amp;nbsp;<a href="http://www.dataminingbook.info/pmwiki.php"><em>Data mining and analysis: fundamental concepts and algorithms</em></a>. &#8594; <strong>ch.1</strong><br/>&raquo; Leskovec, Jure and Rajaraman, Anand and Ullman, Jeffrey David;&amp;nbsp;<a href="http://www.mmds.org/"><em>Mining of massive datasets</em></a>. &#8594; <strong>ch.1</strong><br/>&raquo; Han, Jiawei and Pei, Jian and Kamber, Micheline;&amp;nbsp;<a href="https://ia800300.us.archive.org/5/items/DataMiningConceptAndTechniques2ndEdition/Data.Mining.Concepts.and.Techniques.2nd.Ed-1558609016.pdf"><em>Data mining: concepts and techniques</em></a>. &#8594; <strong>ch.1, ch.2</strong><br/><br/><strong>OPTIONAL</strong><br/>&rsaquo; Downey, Allen;&amp;nbsp;<a href="http://www.greenteapress.com/thinkpython/thinkpython.html"><em>Think Python</em></a>. &#8594; <strong>ch.1-ch.3</strong><br/>&rsaquo; (website) 2017&amp;nbsp;<em>The Periodic Table of Data Science</em><a href="https://www.datacamp.com/community/blog/data-science-periodic-table#gs.TF297Gsm">&amp;nbsp; (https://www.datacamp.com/community/blog/data-science-periodic-table#gs.TF297Gsm)</a>. &#8594; <strong>Familiarize yourself with the entire table.</strong><br/></td>
</tr>

<tr>
<td align="right"><strong>Homework</strong></td>
<td align="left"><strong>DUE:</strong> Monday, <sup>9</sup>&frasl;<sub>3</sub> - midnight<br/>Please see the  Blackboard/<a href="https://github.com/kmsaumcis/mcis6273_f17_datamining">Github repo</a> for what to turn in.</td>
</tr>
</tbody>
</table>

<p><br/><br/></p>

<h2 id="lecture-2-data-representation-preparation-and-manipulation">LECTURE 2: DATA / REPRESENTATION, PREPARATION AND MANIPULATION</h2>

<table>
<thead>
<tr>
<th align="right">Week of <sup>9</sup>&frasl;<sub>6</sub></th>
<th align="left"><a href="https://github.com/kmsaumcis/mcis6273_f17_datamining/tree/master/lecture_notes">Lecture Notes</a></th>
</tr>
</thead>

<tbody>
<tr>
<td align="right"><strong>Content</strong></td>
<td align="left">introduction to core concepts in data; data types and representation of data; data formats including structured and unstructured; concepts in pre-processing data including scaling, sampling, normalizing, binning and imputing</td>
</tr>

<tr>
<td align="right"><strong>Expected<br/>Outcomes</strong></td>
<td align="left">&#8226; understand data types and common formats<br/>&#8226; identify cleaning and adjusting scenarios and apply techniques appropriately<br/>&#8226; utilize and apply the appropriate Python tools (Pandas for data import and cleaning)<br/></td>
</tr>

<tr>
<td align="right"><strong>Readings &amp;<br/>Supplemental</strong></td>
<td align="left"><strong>REQUIRED</strong><br/>&raquo; Zaki, Mohammed J and Meira Jr, Wagner;&amp;nbsp;<a href="http://www.dataminingbook.info/pmwiki.php"><em>Data mining and analysis: fundamental concepts and algorithms</em></a>. &#8594; <strong>ch.1</strong><br/>&raquo; Han, Jiawei and Pei, Jian and Kamber, Micheline;&amp;nbsp;<a href="https://ia800300.us.archive.org/5/items/DataMiningConceptAndTechniques2ndEdition/Data.Mining.Concepts.and.Techniques.2nd.Ed-1558609016.pdf"><em>Data mining: concepts and techniques</em></a>. &#8594; <strong>ch.1, ch.2</strong><br/>&raquo; McKinney, Wes;&amp;nbsp;[<em>Python for data analysis: Data wrangling with Pandas, NumPy, and IPython</em>](). &#8594; <strong>ipython/Jupyter notebooks for ch.5, ch.6 and ch.7</strong><br/>&raquo; (website) 2017&amp;nbsp;<em>Distance computations (scipy.spatial.distance)</em><a href="https://docs.scipy.org/doc/scipy/reference/spatial.distance.html">&amp;nbsp; (https://docs.scipy.org/doc/scipy/reference/spatial.distance.html)</a>. &#8594; <strong>euclidean, cosine, correlation, jaccard</strong><br/><br/><strong>OPTIONAL</strong><br/>&rsaquo; Downey, Allen;&amp;nbsp;<a href="http://www.greenteapress.com/thinkpython/thinkpython.html"><em>Think Python</em></a>. &#8594; <strong>ch.1-ch.3</strong><br/>&rsaquo; (podcast)&amp;nbsp;11-28-2016&amp;nbsp;<em>Episode #90: Data Wrangling with Python</em><a href="http://talkpythontome.fm"> (http://talkpythontome.fm)</a>. &#8594; <strong>listen to the entire episode</strong><br/></td>
</tr>

<tr>
<td align="right"><strong>Homework</strong></td>
<td align="left"><strong>DUE:</strong> Monday, <sup>9</sup>&frasl;<sub>18</sub> - midnight<br/>Please see the  Blackboard/<a href="https://github.com/kmsaumcis/mcis6273_f17_datamining">Github repo</a> for what to turn in.</td>
</tr>
</tbody>
</table>

<p><br/><br/></p>

<h2 id="lecture-3-data-distance-similarity-statistical-concepts">LECTURE 3: DATA / DISTANCE, SIMILARITY, STATISTICAL CONCEPTS</h2>

<table>
<thead>
<tr>
<th align="right">Week of <sup>9</sup>&frasl;<sub>13</sub></th>
<th align="left"><a href="https://github.com/kmsaumcis/mcis6123_fa17/tree/master/lecture_notes">Lecture Notes</a></th>
</tr>
</thead>

<tbody>
<tr>
<td align="right"><strong>Content</strong></td>
<td align="left">introduction to comparing data using common metrics; introductory concepts in disorder; introductory statistical concepts; intuitions over data dimensionality and common reduction techniques</td>
</tr>

<tr>
<td align="right"><strong>Expected<br/>Outcomes</strong></td>
<td align="left">&#8226; identify common distance metrics and their appropriate contexts<br/>&#8226; understand similarity (and dissimilarity) in data<br/>&#8226; develop intuitions of statistical concepts in correlation, distributions and expect value<br/>&#8226; understand dimensionality reduction via PCA<br/>&#8226; utilize and apply basic statistical tools in Python (Pandas/Numpy)<br/></td>
</tr>

<tr>
<td align="right"><strong>Readings &amp;<br/>Supplemental</strong></td>
<td align="left"><strong>REQUIRED</strong><br/>&raquo; Zaki, Mohammed J and Meira Jr, Wagner;&amp;nbsp;<a href="http://www.dataminingbook.info/pmwiki.php"><em>Data mining and analysis: fundamental concepts and algorithms</em></a>. &#8594; <strong>ch.7</strong><br/>&raquo; Leskovec, Jure and Rajaraman, Anand and Ullman, Jeffrey David;&amp;nbsp;<a href="http://www.mmds.org/"><em>Mining of massive datasets</em></a>. &#8594; <strong>ch.11</strong><br/>&raquo; Han, Jiawei and Pei, Jian and Kamber, Micheline;&amp;nbsp;<a href="https://ia800300.us.archive.org/5/items/DataMiningConceptAndTechniques2ndEdition/Data.Mining.Concepts.and.Techniques.2nd.Ed-1558609016.pdf"><em>Data mining: concepts and techniques</em></a>. &#8594; <strong>ch.2.5, ch.10.4.2</strong><br/>&raquo; VanderPlas, Jake;&amp;nbsp;<a href="https://github.com/jakevdp/PythonDataScienceHandbook"><em>Python Data Science Handbook</em></a>. &#8594; <strong>ch.5.10 (In-depth Principal Components Analysis notebook)</strong><br/>&raquo; (website) 2017&amp;nbsp;<em>sklearn.neighbors.DistanceMetric class</em><a href="http://scikit-learn.org/stable/modules/generated/sklearn.neighbors.DistanceMetric.html">&amp;nbsp; (http://scikit-learn.org/stable/modules/generated/sklearn.neighbors.DistanceMetric.html)</a>. &#8594; <strong>euclidean, cosine, jaccard</strong><br/><br/><strong>OPTIONAL</strong><br/>&rsaquo; (website) 2017&amp;nbsp;<em>Distance computations (scipy.spatial.distance)</em><a href="https://docs.scipy.org/doc/scipy/reference/spatial.distance.html">&amp;nbsp; (https://docs.scipy.org/doc/scipy/reference/spatial.distance.html)</a>. &#8594; <strong>cdist, euclidean, cosine, jaccard</strong><br/></td>
</tr>

<tr>
<td align="right"><strong>Homework</strong></td>
<td align="left">&ndash;</td>
</tr>
</tbody>
</table>

<p><br/><br/></p>

<h2 id="lecture-4-association-rule-mining-pattern-mining">LECTURE 4: ASSOCIATION RULE MINING, PATTERN MINING</h2>

<table>
<thead>
<tr>
<th align="right">Week of <sup>9</sup>&frasl;<sub>20</sub></th>
<th align="left"><a href="https://github.com/kmsaumcis/mcis6273_f17_datamining/tree/master/lecture_notes">Lecture Notes</a></th>
</tr>
</thead>

<tbody>
<tr>
<td align="right"><strong>Content</strong></td>
<td align="left">introduction to concepts for rule and pattern mining; introdcution to apriori algorithm for frequent patterns; motivating the market basket analysis context for pattern mining; exploring addition contexts</td>
</tr>

<tr>
<td align="right"><strong>Expected<br/>Outcomes</strong></td>
<td align="left">&#8226; understand concepts behind frequent patterns<br/>&#8226; understand association rule mining, apriori algorithm, FP-growth<br/>&#8226; apply and compute basic patterns by hand<br/>&#8226; identify the contexts for applying pattern mining<br/></td>
</tr>

<tr>
<td align="right"><strong>Readings &amp;<br/>Supplemental</strong></td>
<td align="left"><strong>REQUIRED</strong><br/>&raquo; Zaki, Mohammed J and Meira Jr, Wagner;&amp;nbsp;<a href="http://www.dataminingbook.info/pmwiki.php"><em>Data mining and analysis: fundamental concepts and algorithms</em></a>. &#8594; <strong>ch.8</strong><br/>&raquo; Han, Jiawei and Pei, Jian and Kamber, Micheline;&amp;nbsp;<a href="https://ia800300.us.archive.org/5/items/DataMiningConceptAndTechniques2ndEdition/Data.Mining.Concepts.and.Techniques.2nd.Ed-1558609016.pdf"><em>Data mining: concepts and techniques</em></a>. &#8594; <strong>ch.5</strong><br/></td>
</tr>

<tr>
<td align="right"><strong>Homework</strong></td>
<td align="left"><strong>DUE:</strong> Monday, <sup>10</sup>&frasl;<sub>2</sub> - midnight<br/>Please see the  Blackboard/<a href="https://github.com/kmsaumcis/mcis6273_f17_datamining">Github repo</a> for what to turn in.</td>
</tr>
</tbody>
</table>

<p><br/><br/></p>

<h2 id="lecture-5-unsupervised-techniques-introduction-to-clustering">LECTURE 5: UNSUPERVISED TECHNIQUES / INTRODUCTION TO CLUSTERING</h2>

<table>
<thead>
<tr>
<th align="right">Week of <sup>9</sup>&frasl;<sub>27</sub></th>
<th align="left"><a href="https://github.com/kmsaumcis/mcis6273_f17_datamining/tree/master/lecture_notes">Lecture Notes</a></th>
</tr>
</thead>

<tbody>
<tr>
<td align="right"><strong>Content</strong></td>
<td align="left">introduction to cluster analysis and motivations; introduction to unsupervised clustering algorithms; partitioning (k-means, k-mediods); hierarchical agglomerative methods; model-based (expectation-maximization) neural networks (SOM self-organizing maps); visualing with voronoi diagrams</td>
</tr>

<tr>
<td align="right"><strong>Expected<br/>Outcomes</strong></td>
<td align="left">&#8226; exposure to unsupervised clustering methods, k-Means<br/>&#8226; introduction to key clustering algorithms<br/>&#8226; distinguish between partition and model-based algorithms<br/></td>
</tr>

<tr>
<td align="right"><strong>Readings &amp;<br/>Supplemental</strong></td>
<td align="left"><strong>REQUIRED</strong><br/>&raquo; Zaki, Mohammed J and Meira Jr, Wagner;&amp;nbsp;<a href="http://www.dataminingbook.info/pmwiki.php"><em>Data mining and analysis: fundamental concepts and algorithms</em></a>. &#8594; <strong>ch.13, ch.14, ch.15, ch.17</strong><br/>&raquo; Han, Jiawei and Pei, Jian and Kamber, Micheline;&amp;nbsp;<a href="https://ia800300.us.archive.org/5/items/DataMiningConceptAndTechniques2ndEdition/Data.Mining.Concepts.and.Techniques.2nd.Ed-1558609016.pdf"><em>Data mining: concepts and techniques</em></a>. &#8594; <strong>ch.7</strong><br/>&raquo; (website) 2015&amp;nbsp;<em>Basic Clustering with k-Means</em><a href="https://nbviewer.jupyter.org/github/tmbdev/teaching-mmir/blob/master/30-kmeans.ipynb">&amp;nbsp; (https://nbviewer.jupyter.org/github/tmbdev/teaching-mmir/blob/master/30-kmeans.ipynb)</a>. &#8594; <strong>Familiarize yourself with the notebook.</strong><br/></td>
</tr>

<tr>
<td align="right"><strong>Homework</strong></td>
<td align="left"><strong>DUE:</strong> Monday, <sup>10</sup>&frasl;<sub>23</sub> - midnight<br/>Please see the  Blackboard/<a href="https://github.com/kmsaumcis/mcis6273_f17_datamining">Github repo</a> for what to turn in.</td>
</tr>
</tbody>
</table>

<p><br/><br/></p>

<h2 id="lecture-6-unsupervised-techniques-more-clustering">LECTURE 6: UNSUPERVISED TECHNIQUES / MORE CLUSTERING</h2>

<table>
<thead>
<tr>
<th align="right">Week of <sup>10</sup>&frasl;<sub>4</sub></th>
<th align="left"><a href="https://github.com/kmsaumcis/mcis6273_f17_datamining/tree/master/lecture_notes">Lecture Notes</a></th>
</tr>
</thead>

<tbody>
<tr>
<td align="right"><strong>Content</strong></td>
<td align="left">continued clustering, hierachical algorithms (agglomorative), introduction to density-based algorithms (DBSCAN)</td>
</tr>

<tr>
<td align="right"><strong>Expected<br/>Outcomes</strong></td>
<td align="left">&#8226; understand hierarchical and density-based algorithms<br/>&#8226; develop intuitions for choosing algorithms in various contexts<br/>&#8226; utilize algorithms on read-world data<br/></td>
</tr>

<tr>
<td align="right"><strong>Readings &amp;<br/>Supplemental</strong></td>
<td align="left">No assigned readings. Please complete readings from previous week if not current.</td>
</tr>

<tr>
<td align="right"><strong>Homework</strong></td>
<td align="left">&ndash;</td>
</tr>
</tbody>
</table>

<p><br/><br/></p>

<h2 id="lecture-7-supervised-techniques-classification-and-prediction">LECTURE 7: SUPERVISED TECHNIQUES / CLASSIFICATION AND PREDICTION</h2>

<table>
<thead>
<tr>
<th align="right">Week of <sup>10</sup>&frasl;<sub>11</sub></th>
<th align="left"><a href="https://github.com/kmsaumcis/mcis6273_f17_datamining/tree/master/lecture_notes">Lecture Notes</a></th>
</tr>
</thead>

<tbody>
<tr>
<td align="right"><strong>Content</strong></td>
<td align="left">classification and prediction; understanding decision trees, concepts and theory; probabilistic approaches to classification - na&iuml;ve bayes; introduction to bayesian belief networks</td>
</tr>

<tr>
<td align="right"><strong>Expected<br/>Outcomes</strong></td>
<td align="left">&#8226; understand and explain decision trees<br/>&#8226; develop probabilistic models of classification using na&iuml;ve Bayes<br/>&#8226; identify BBNs and their application context<br/>&#8226; utilize na&iuml;ve Bayes in real-world applications<br/></td>
</tr>

<tr>
<td align="right"><strong>Readings &amp;<br/>Supplemental</strong></td>
<td align="left"><strong>REQUIRED</strong><br/>&raquo; Zaki, Mohammed J and Meira Jr, Wagner;&amp;nbsp;<a href="http://www.dataminingbook.info/pmwiki.php"><em>Data mining and analysis: fundamental concepts and algorithms</em></a>. &#8594; <strong>ch.18, ch.19</strong><br/>&raquo; Han, Jiawei and Pei, Jian and Kamber, Micheline;&amp;nbsp;<a href="https://ia800300.us.archive.org/5/items/DataMiningConceptAndTechniques2ndEdition/Data.Mining.Concepts.and.Techniques.2nd.Ed-1558609016.pdf"><em>Data mining: concepts and techniques</em></a>. &#8594; <strong>ch.6.3, ch.6.4</strong><br/></td>
</tr>

<tr>
<td align="right"><strong>Homework</strong></td>
<td align="left"><strong>DUE:</strong> Monday, <sup>11</sup>&frasl;<sub>3</sub> - midnight<br/>Please see the  Blackboard/<a href="https://github.com/kmsaumcis/mcis6273_f17_datamining">Github repo</a> for what to turn in.</td>
</tr>
</tbody>
</table>

<p><br/><br/></p>

<h2 id="lecture-8-supervised-techniques-classification-and-prediction">LECTURE 8: SUPERVISED TECHNIQUES / CLASSIFICATION AND PREDICTION</h2>

<table>
<thead>
<tr>
<th align="right">Week of <sup>10</sup>&frasl;<sub>18</sub></th>
<th align="left"><a href="https://github.com/kmsaumcis/mcis6273_f17_datamining/tree/master/lecture_notes">Lecture Notes</a></th>
</tr>
</thead>

<tbody>
<tr>
<td align="right"><strong>Content</strong></td>
<td align="left">linear regression  models for prediction; logistic regression models for prediction; introduction to generalized linear models</td>
</tr>

<tr>
<td align="right"><strong>Expected<br/>Outcomes</strong></td>
<td align="left">&#8226; understand and develop linear regression models<br/>&#8226; understand and interpret logistic regression models<br/>&#8226; exposure to generalized linear models<br/></td>
</tr>

<tr>
<td align="right"><strong>Readings &amp;<br/>Supplemental</strong></td>
<td align="left"><strong>REQUIRED</strong><br/>&raquo; Zaki, Mohammed J and Meira Jr, Wagner;&amp;nbsp;<a href="http://www.dataminingbook.info/pmwiki.php"><em>Data mining and analysis: fundamental concepts and algorithms</em></a>. &#8594; <strong>ch.20</strong><br/>&raquo; Han, Jiawei and Pei, Jian and Kamber, Micheline;&amp;nbsp;<a href="https://ia800300.us.archive.org/5/items/DataMiningConceptAndTechniques2ndEdition/Data.Mining.Concepts.and.Techniques.2nd.Ed-1558609016.pdf"><em>Data mining: concepts and techniques</em></a>. &#8594; <strong>ch.6.11</strong><br/></td>
</tr>

<tr>
<td align="right"><strong>Homework</strong></td>
<td align="left">&ndash;</td>
</tr>
</tbody>
</table>

<p><br/><br/></p>

<h2 id="lecture-9-supervised-techniques-classification-and-model-evaluation">LECTURE 9: SUPERVISED TECHNIQUES / CLASSIFICATION AND MODEL EVALUATION</h2>

<table>
<thead>
<tr>
<th align="right">Week of <sup>10</sup>&frasl;<sub>25</sub></th>
<th align="left"><a href="https://github.com/kmsaumcis/mcis6273_f17_datamining/tree/master/lecture_notes">Lecture Notes</a></th>
</tr>
</thead>

<tbody>
<tr>
<td align="right"><strong>Content</strong></td>
<td align="left">support vector machines; neural networks and the basic NN model and its relation to learning algorithms; evaluating models and applying techniques to model validation</td>
</tr>

<tr>
<td align="right"><strong>Expected<br/>Outcomes</strong></td>
<td align="left">&#8226; understand support vector machines and their strengths<br/>&#8226; understand neural networks, their basic theory and application<br/>&#8226; identify and develop intutition around model evaluation and validation<br/></td>
</tr>

<tr>
<td align="right"><strong>Readings &amp;<br/>Supplemental</strong></td>
<td align="left"><strong>REQUIRED</strong><br/>&raquo; Zaki, Mohammed J and Meira Jr, Wagner;&amp;nbsp;<a href="http://www.dataminingbook.info/pmwiki.php"><em>Data mining and analysis: fundamental concepts and algorithms</em></a>. &#8594; <strong>ch.21</strong><br/>&raquo; Han, Jiawei and Pei, Jian and Kamber, Micheline;&amp;nbsp;<a href="https://ia800300.us.archive.org/5/items/DataMiningConceptAndTechniques2ndEdition/Data.Mining.Concepts.and.Techniques.2nd.Ed-1558609016.pdf"><em>Data mining: concepts and techniques</em></a>. &#8594; <strong>ch.6.6, ch.6.7</strong><br/></td>
</tr>

<tr>
<td align="right"><strong>Homework</strong></td>
<td align="left"><strong>DUE:</strong> Monday, <sup>11</sup>&frasl;<sub>30</sub> - midnight<br/>Please see the  Blackboard/<a href="https://github.com/kmsaumcis/mcis6273_f17_datamining">Github repo</a> for what to turn in.</td>
</tr>
</tbody>
</table>

<p><br/><br/></p>

<h2 id="lecture-10-ensemble-methods">LECTURE 10: ENSEMBLE METHODS</h2>

<table>
<thead>
<tr>
<th align="right">Week of <sup>11</sup>&frasl;<sub>1</sub></th>
<th align="left"><a href="https://github.com/kmsaumcis/mcis6273_f17_datamining/tree/master/lecture_notes">Lecture Notes</a></th>
</tr>
</thead>

<tbody>
<tr>
<td align="right"><strong>Content</strong></td>
<td align="left">ensemble methods; introduction to boosting, bagging, random forests and related methods</td>
</tr>

<tr>
<td align="right"><strong>Expected<br/>Outcomes</strong></td>
<td align="left">&#8226; understand and identify the need for ensembles<br/>&#8226; identify and develop intutition around ensemble model evaluation and validation<br/></td>
</tr>

<tr>
<td align="right"><strong>Readings &amp;<br/>Supplemental</strong></td>
<td align="left"><strong>REQUIRED</strong><br/>&raquo; Zaki, Mohammed J and Meira Jr, Wagner;&amp;nbsp;<a href="http://www.dataminingbook.info/pmwiki.php"><em>Data mining and analysis: fundamental concepts and algorithms</em></a>. &#8594; <strong>ch.22</strong><br/>&raquo; Han, Jiawei and Pei, Jian and Kamber, Micheline;&amp;nbsp;<a href="https://ia800300.us.archive.org/5/items/DataMiningConceptAndTechniques2ndEdition/Data.Mining.Concepts.and.Techniques.2nd.Ed-1558609016.pdf"><em>Data mining: concepts and techniques</em></a>. &#8594; <strong>ch.6.12, ch.6.13, ch.6.14, ch.6.15</strong><br/></td>
</tr>

<tr>
<td align="right"><strong>Homework</strong></td>
<td align="left">&ndash;</td>
</tr>
</tbody>
</table>

<p><br/><br/></p>

<h2 id="lecture-11-data-visualization-introductory-concepts">LECTURE 11: DATA VISUALIZATION: INTRODUCTORY CONCEPTS</h2>

<table>
<thead>
<tr>
<th align="right">Week of <sup>11</sup>&frasl;<sub>8</sub></th>
<th align="left"><a href="https://github.com/kmsaumcis/mcis6273_f17_datamining/tree/master/lecture_notes">Lecture Notes</a></th>
</tr>
</thead>

<tbody>
<tr>
<td align="right"><strong>Content</strong></td>
<td align="left">introduction to data visualization; building data narratives</td>
</tr>

<tr>
<td align="right"><strong>Expected<br/>Outcomes</strong></td>
<td align="left">&#8226; understand core social mining algorithms<br/>&#8226; understand concepts in network analysis<br/></td>
</tr>

<tr>
<td align="right"><strong>Readings &amp;<br/>Supplemental</strong></td>
<td align="left"><strong>REQUIRED</strong><br/>&raquo; Knaflic, Cole Nussbaumer;&amp;nbsp;[<em>Storytelling with data: A data visualization guide for business professionals</em>](). &#8594; <strong>ch.8</strong><br/><br/><strong>OPTIONAL</strong><br/>&rsaquo; B\&ldquo;orner, Katy and Polley, David E;&amp;nbsp;[<em>Visual insights: A practical guide to making sense of data</em>](). &#8594; <strong>ch.5</strong><br/>&rsaquo; Curley, James P.;&amp;nbsp;<a href="http://rpubs.com/jalapic/scrabblr"><em>Analyzing Scrabble Games</em></a>. &#8594; <strong>This is a very interesting exploration in analysis and visualization.</strong><br/></td>
</tr>

<tr>
<td align="right"><strong>Homework</strong></td>
<td align="left">&ndash;</td>
</tr>
</tbody>
</table>

<p><br/><br/></p>

<h2 id="lecture-12-introduction-to-social-mining">LECTURE 12: INTRODUCTION TO SOCIAL MINING</h2>

<table>
<thead>
<tr>
<th align="right">Week of <sup>11</sup>&frasl;<sub>15</sub></th>
<th align="left"><a href="https://github.com/kmsaumcis/mcis6273_f17_datamining/tree/master/lecture_notes">Lecture Notes</a></th>
</tr>
</thead>

<tbody>
<tr>
<td align="right"><strong>Content</strong></td>
<td align="left">introduction to social mining; introduction to recommendation systems, collaborative and content-based filtering</td>
</tr>

<tr>
<td align="right"><strong>Expected<br/>Outcomes</strong></td>
<td align="left">&#8226; understand core social mining algorithms<br/>&#8226; understand concepts in network analysis<br/>&#8226; understand core recommender system concepts<br/></td>
</tr>

<tr>
<td align="right"><strong>Readings &amp;<br/>Supplemental</strong></td>
<td align="left"><strong>REQUIRED</strong><br/>&raquo; Leskovec, Jure and Rajaraman, Anand and Ullman, Jeffrey David;&amp;nbsp;<a href="http://www.mmds.org/"><em>Mining of massive datasets</em></a>. &#8594; <strong>ch.10</strong><br/>&raquo; Grus, Joel;&amp;nbsp;[<em>Data science from scratch: First principles with Python</em>](). &#8594; <strong>ch.22</strong><br/><br/><strong>OPTIONAL</strong><br/>&rsaquo; Leskovec, Jure and Rajaraman, Anand and Ullman, Jeffrey David;&amp;nbsp;<a href="http://www.mmds.org/"><em>Mining of massive datasets</em></a>. &#8594; <strong>ch.9</strong><br/>&rsaquo; B\&ldquo;orner, Katy and Polley, David E;&amp;nbsp;[<em>Visual insights: A practical guide to making sense of data</em>](). &#8594; <strong>ch.5</strong><br/></td>
</tr>

<tr>
<td align="right"><strong>Homework</strong></td>
<td align="left">&ndash;</td>
</tr>
</tbody>
</table>

<p><br/><br/></p>

<h2 id="lecture-13-introduction-to-text-mining">LECTURE 13: INTRODUCTION TO TEXT MINING</h2>

<table>
<thead>
<tr>
<th align="right">Week of <sup>11</sup>&frasl;<sub>29</sub></th>
<th align="left"><a href="https://github.com/kmsaumcis/mcis6273_f17_datamining/tree/master/lecture_notes">Lecture Notes</a></th>
</tr>
</thead>

<tbody>
<tr>
<td align="right"><strong>Content</strong></td>
<td align="left">introduction to text mining; concepts in document preparation pipeline (tokenizing, stemming, etc.); TFIDF, cosine similarity; corpus selection</td>
</tr>

<tr>
<td align="right"><strong>Expected<br/>Outcomes</strong></td>
<td align="left">&#8226; understand introductory concepts in text mining and information retrieval<br/>&#8226; understand document preparation tools<br/>&#8226; apply basic concepts to real-world data<br/></td>
</tr>

<tr>
<td align="right"><strong>Readings &amp;<br/>Supplemental</strong></td>
<td align="left"><strong>REQUIRED</strong><br/>&raquo; Han, Jiawei and Pei, Jian and Kamber, Micheline;&amp;nbsp;<a href="https://ia800300.us.archive.org/5/items/DataMiningConceptAndTechniques2ndEdition/Data.Mining.Concepts.and.Techniques.2nd.Ed-1558609016.pdf"><em>Data mining: concepts and techniques</em></a>. &#8594; <strong>ch.10.4</strong><br/>&raquo; Manning, Christopher D and Raghavan, Prabhakar and Sch\&ldquo;utze, Hinrich;&amp;nbsp;<a href="https://nlp.stanford.edu/IR-book/information-retrieval-book.html"><em>Introduction to information retrieval</em></a>. &#8594; <strong>ch.6</strong><br/><br/><strong>OPTIONAL</strong><br/>&rsaquo; Manning, Christopher D and Raghavan, Prabhakar and Sch\&ldquo;utze, Hinrich;&amp;nbsp;<a href="https://nlp.stanford.edu/IR-book/information-retrieval-book.html"><em>Introduction to information retrieval</em></a>. &#8594; <strong>ch.13</strong><br/></td>
</tr>

<tr>
<td align="right"><strong>Homework</strong></td>
<td align="left">&ndash;</td>
</tr>
</tbody>
</table>

<p><br/><br/></p>

<h2 id="lecture-14-open-data-ethics-in-data-mining-the-future-of-data-science">LECTURE 14**: OPEN DATA, ETHICS IN DATA MINING, THE FUTURE OF DATA SCIENCE</h2>

<table>
<thead>
<tr>
<th align="right">Week of <sup>12</sup>&frasl;<sub>6</sub></th>
<th align="left"><a href="https://github.com/kmsaumcis/mcis6273_f17_datamining/tree/master/lecture_notes">Lecture Notes</a></th>
</tr>
</thead>

<tbody>
<tr>
<td align="right"><strong>Content</strong></td>
<td align="left">open data portals, APIs, tools and technologies; ethics in data mining; anonymization, privacy and data considerations; data science and the future</td>
</tr>

<tr>
<td align="right"><strong>Expected<br/>Outcomes</strong></td>
<td align="left">&#8226; exposure to open data portals and open data technologies<br/>&#8226; exposure to open APIs and tools for open data access<br/>&#8226; understand data mining ethics and why ethics (and privacy) are critically important<br/>&#8226; the future to data science, analytics and intelligent systems built on big data<br/></td>
</tr>

<tr>
<td align="right"><strong>Readings &amp;<br/>Supplemental</strong></td>
<td align="left">No assigned readings. Please complete readings from previous week if not current.</td>
</tr>

<tr>
<td align="right"><strong>Homework</strong></td>
<td align="left">&ndash;</td>
</tr>
</tbody>
</table>

<p><br/><br/></p>

<h2 id="resources">RESOURCES</h2>

<ol>
<li>Zaki, Mohammed J and Meira Jr, Wagner;&amp;nbsp;<a href="http://www.dataminingbook.info/pmwiki.php"><em>Data mining and analysis: fundamental concepts and algorithms</em></a>.<br/></li>
<li>Leskovec, Jure and Rajaraman, Anand and Ullman, Jeffrey David;&amp;nbsp;<a href="http://www.mmds.org/"><em>Mining of massive datasets</em></a>.<br/></li>
<li>Yau, Nathan;&amp;nbsp;<a href="http://book.flowingdata.com/"><em>Visualize this: the FlowingData guide to design, visualization, and statistics</em></a>.<br/></li>
<li>B{\&ldquo;o}rner, Katy and Polley, David E;&amp;nbsp;[<em>Visual insights: A practical guide to making sense of data</em>]().<br/></li>
<li>Downey, Allen;&amp;nbsp;<a href="http://www.greenteapress.com/thinkpython/thinkpython.html"><em>Think Python</em></a>.<br/></li>
<li>Segaran, Toby;&amp;nbsp;[<em>Programming collective intelligence: building smart web 2.0 applications</em>]().<br/></li>
<li>Conway, Drew and White, John;&amp;nbsp;[<em>Machine learning for hackers</em>]().<br/></li>
<li>Grus, Joel;&amp;nbsp;[<em>Data science from scratch: First principles with Python</em>]().<br/></li>
<li>(website) 2017&amp;nbsp;<em>The Periodic Table of Data Science</em><a href="https://www.datacamp.com/community/blog/data-science-periodic-table#gs.TF297Gsm">&amp;nbsp; (https://www.datacamp.com/community/blog/data-science-periodic-table#gs.TF297Gsm)</a>.<br/></li>
<li>Han, Jiawei and Pei, Jian and Kamber, Micheline;&amp;nbsp;<a href="https://ia800300.us.archive.org/5/items/DataMiningConceptAndTechniques2ndEdition/Data.Mining.Concepts.and.Techniques.2nd.Ed-1558609016.pdf"><em>Data mining: concepts and techniques</em></a>.<br/></li>
<li>McKinney, Wes;&amp;nbsp;[<em>Python for data analysis: Data wrangling with Pandas, NumPy, and IPython</em>]().<br/></li>
<li>Manning, Christopher D and Raghavan, Prabhakar and Sch{\&ldquo;u}tze, Hinrich;&amp;nbsp;<a href="https://nlp.stanford.edu/IR-book/information-retrieval-book.html"><em>Introduction to information retrieval</em></a>.<br/></li>
<li>Knaflic, Cole Nussbaumer;&amp;nbsp;[<em>Storytelling with data: A data visualization guide for business professionals</em>]().<br/></li>
<li>Rose, Doug;&amp;nbsp;[<em>Data Science: Create Teams That Ask the Right Questions and Deliver Real Value</em>]().<br/></li>
<li>(website) 2013&amp;nbsp;<em>Mining the Social Web: Data Mining Facebook, Twitter, LinkedIn, Google+, GitHub, and More</em><a href="https://github.com/ptwobrussell/Mining-the-Social-Web-2nd-Edition/">&amp;nbsp; (https://github.com/ptwobrussell/Mining-the-Social-Web-2nd-Edition/)</a>.<br/></li>
<li>Wexler, Steve and Shaffer, Jeffrey and Cotgreave, Andy;&amp;nbsp;[<em>The Big Book of Dashboards: Visualizing Your Data Using Real-World Business Scenarios</em>]().<br/></li>
<li>VanderPlas, Jake;&amp;nbsp;<a href="https://github.com/jakevdp/PythonDataScienceHandbook"><em>Python Data Science Handbook</em></a>.<br/></li>
<li>(website) 2015&amp;nbsp;<em>Basic Clustering with k-Means</em><a href="https://nbviewer.jupyter.org/github/tmbdev/teaching-mmir/blob/master/30-kmeans.ipynb">&amp;nbsp; (https://nbviewer.jupyter.org/github/tmbdev/teaching-mmir/blob/master/30-kmeans.ipynb)</a>.<br/></li>
<li>(website) 2017&amp;nbsp;<em>Distance computations (scipy.spatial.distance)</em><a href="https://docs.scipy.org/doc/scipy/reference/spatial.distance.html">&amp;nbsp; (https://docs.scipy.org/doc/scipy/reference/spatial.distance.html)</a>.<br/></li>
<li>(website) 11-15-2016&amp;nbsp;<em>Jupyter Notebook Tutorial: The Definitive Guide</em><a href="https://www.datacamp.com/community/tutorials/tutorial-jupyter-notebook#gs.zExWvMw">&amp;nbsp; (https://www.datacamp.com/community/tutorials/tutorial-jupyter-notebook#gs.zExWvMw)</a>.<br/></li>
<li>(website) 2017&amp;nbsp;<em>sklearn.neighbors.DistanceMetric class</em><a href="http://scikit-learn.org/stable/modules/generated/sklearn.neighbors.DistanceMetric.html">&amp;nbsp; (http://scikit-learn.org/stable/modules/generated/sklearn.neighbors.DistanceMetric.html)</a>.<br/></li>
<li>(podcast)&amp;nbsp;11-28-2016&amp;nbsp;<em>Episode #90: Data Wrangling with Python</em><a href="http://talkpythontome.fm"> (http://talkpythontome.fm)</a>.<br/></li>
<li>Curley, James P.;&amp;nbsp;<a href="http://rpubs.com/jalapic/scrabblr"><em>Analyzing Scrabble Games</em></a>.<br/></li>
</ol>

</main>

  <footer>
  <script src="//yihui.name/js/math-code.js"></script>
<script async src="//cdn.bootcss.com/mathjax/2.7.1/MathJax.js?config=TeX-MML-AM_CHTML"></script>

<script async src="//yihui.name/js/center-img.js"></script>

  
  </footer>
  </body>
</html>

