<!DOCTYPE html>
<html lang="en-us">
  <head>
    <meta charset="utf-8">
    <meta name="viewport" content="width=device-width, initial-scale=1">
    <title> | MCIS6273 | Data Mining | Fall 2018</title>
    <link rel="stylesheet" href="/mcis6273_f18_datamining/css/style.css" />
    <link rel="stylesheet" href="/mcis6273_f18_datamining/css/fonts.css" />
    <style>
@import url('https://fonts.googleapis.com/css?family=Fanwood+Text:400,400i|Faustina:400,400i,600,600i|Libre+Baskerville|Lora:400,400i,700,700i|Merriweather:400,400i,700,700i|Old+Standard+TT|Open+Sans:400,400i,700,700i|Playfair+Display|Radley|Spectral:400,400i,600,600i,700,700i');
</style>

  </head>

  <body>
    <nav>
    <ul class="menu">
      
      <li><a href="/mcis6273_f18_datamining">Home</a></li>
      
      <li><a href="/mcis6273_f18_datamining/syllabus/">Syllabus</a></li>
      
      <li><a href="/mcis6273_f18_datamining/homework/">Homework</a></li>
      
      <li><a href="/mcis6273_f18_datamining/index.xml">Subscribe</a></li>
      
    </ul>
    <hr/>
    </nav>

<div class="article-meta">
<h1><span class="title"></span></h1>


</div>

<main>


<h1 id="mcis6273-data-mining-fall-2018-prof-maull">MCIS6273 Data Mining / Fall 2018 / Prof. Maull</h1>

<h2 id="lecture-1-class-policies-tools-and-technologies">LECTURE 1: CLASS POLICIES, TOOLS AND TECHNOLOGIES</h2>

<table>
<thead>
<tr>
<th align="right">Week of <sup>8</sup>&frasl;<sub>27</sub></th>
<th align="left"><a href="https://github.com/kmsaumcis/mcis6273_f18_datamining/tree/master/lecture_notes/">Lecture Notes</a></th>
</tr>
</thead>

<tbody>
<tr>
<td align="right"><strong>Content</strong></td>
<td align="left">class policies, class tools, introduction, what this course is about, data mining: tools, technologies and techniques</td>
</tr>

<tr>
<td align="right"><strong>Expected<br/>Outcomes</strong></td>
<td align="left">&#8226; overview of course policies<br/>&#8226; overview of data mining concepts, algorithms, methodologies<br/>&#8226; installation of Anaconda and Python 3.6<br/>&#8226; introduction to Jupyter Notebooks<br/>&#8226; creation of Github account<br/></td>
</tr>

<tr>
<td align="right"><strong>Readings &amp;<br/>Supplemental</strong></td>
<td align="left"><strong>REQUIRED</strong><br/>&raquo; 2014. Zaki, Mohammed J and Meira Jr, Wagner; <a href="http://www.dataminingbook.info/pmwiki.php"><em>Data mining and analysis: fundamental concepts and algorithms</em></a>. &#8594; <strong>ch.1</strong><br/>&raquo; 2014. Leskovec, Jure and Rajaraman, Anand and Ullman, Jeffrey David; <a href="http://www.mmds.org/"><em>Mining of massive datasets</em></a>. &#8594; <strong>ch.1</strong><br/>&raquo; 2011. Han, Jiawei and Pei, Jian and Kamber, Micheline; <a href="https://ia800300.us.archive.org/5/items/DataMiningConceptAndTechniques2ndEdition/Data.Mining.Concepts.and.Techniques.2nd.Ed-1558609016.pdf"><em>Data mining: concepts and techniques</em></a>. &#8594; <strong>ch.1, ch.2</strong><br/><br/><strong>OPTIONAL</strong><br/>&rsaquo; 2012. Downey, Allen; <a href="http://www.greenteapress.com/thinkpython/thinkpython.html"><em>Think Python</em></a>. &#8594; <strong>ch.1-ch.3</strong><br/>&rsaquo; (website) -- 2017; <em>The Periodic Table of Data Science</em>: <a href="https://www.datacamp.com/community/blog/data-science-periodic-table#gs.TF297Gsm">https://www.datacamp.com/community/blog/data-science-periodic-table#gs.TF297Gsm</a>. &#8594; <strong>Familiarize yourself with the entire table.</strong><br/></td>
</tr>

<tr>
<td align="right"><strong>Homework</strong></td>
<td align="left"><strong>DUE:</strong> Wednesday, <sup>9</sup>&frasl;<sub>12</sub> - midnight<br/>Please see the Blackboard/<a href="https://github.com/kmsaumcis/mcis6273_f18_datamining/homework">Github repo</a> for what to turn in.</td>
</tr>
</tbody>
</table>

<h2 id="lecture-2-data-representation-preparation-and-manipulation">LECTURE 2: DATA / REPRESENTATION, PREPARATION AND MANIPULATION</h2>

<table>
<thead>
<tr>
<th align="right">Week of <sup>9</sup>&frasl;<sub>3</sub></th>
<th align="left"><a href="https://github.com/kmsaumcis/mcis6273_f18_datamining/tree/master/lecture_notes">Lecture Notes</a></th>
</tr>
</thead>

<tbody>
<tr>
<td align="right"><strong>Content</strong></td>
<td align="left">introduction to core concepts in data; data types and representation of data; data formats including structured and unstructured; concepts in pre-processing data including scaling, sampling, normalizing, binning and imputing</td>
</tr>

<tr>
<td align="right"><strong>Expected<br/>Outcomes</strong></td>
<td align="left">&#8226; understand data types and common formats<br/>&#8226; identify cleaning and adjusting scenarios and apply techniques appropriately<br/>&#8226; utilize and apply the appropriate Python tools (Pandas for data import and cleaning)<br/></td>
</tr>

<tr>
<td align="right"><strong>Readings &amp;<br/>Supplemental</strong></td>
<td align="left"><strong>REQUIRED</strong><br/>&raquo; 2014. Zaki, Mohammed J and Meira Jr, Wagner; <a href="http://www.dataminingbook.info/pmwiki.php"><em>Data mining and analysis: fundamental concepts and algorithms</em></a>. &#8594; <strong>ch.1</strong><br/>&raquo; 2011. Han, Jiawei and Pei, Jian and Kamber, Micheline; <a href="https://ia800300.us.archive.org/5/items/DataMiningConceptAndTechniques2ndEdition/Data.Mining.Concepts.and.Techniques.2nd.Ed-1558609016.pdf"><em>Data mining: concepts and techniques</em></a>. &#8594; <strong>ch.1, ch.2</strong><br/>&raquo; 2012. McKinney, Wes; <a href="https://github.com/wesm/pydata-book"><em>Python for data analysis: Data wrangling with Pandas, NumPy, and IPython</em></a>. &#8594; <strong>ipython/Jupyter notebooks for ch.5, ch.6 and ch.7</strong><br/>&raquo; (website) -- 2017; <em>Distance computations (scipy.spatial.distance)</em>: <a href="https://docs.scipy.org/doc/scipy/reference/spatial.distance.html">https://docs.scipy.org/doc/scipy/reference/spatial.distance.html</a>. &#8594; <strong>euclidean, cosine, correlation, jaccard</strong><br/><br/><strong>OPTIONAL</strong><br/>&rsaquo; 2012. Downey, Allen; <a href="http://www.greenteapress.com/thinkpython/thinkpython.html"><em>Think Python</em></a>. &#8594; <strong>ch.1-ch.3</strong><br/>&rsaquo; (website) -- 2017; <em>Pandas Cookbook</em>: <a href="https://github.com/jvns/pandas-cookbook">https://github.com/jvns/pandas-cookbook</a>. &#8594; <strong>familiarize yourself with this content of this repo</strong><br/>&rsaquo; (Michael Kennedy&rsquo;s Talk Python To Me podcast) -- 11-28-2016; <em>Episode #90: Data Wrangling with Python</em>: <a href="http://talkpythontome.fm">http://talkpythontome.fm</a>. &#8594; <strong>listen to the entire episode</strong><br/></td>
</tr>

<tr>
<td align="right"><strong>Homework</strong></td>
<td align="left">&ndash;</td>
</tr>
</tbody>
</table>

<h2 id="lecture-3-data-distance-similarity-statistical-concepts">LECTURE 3: DATA / DISTANCE, SIMILARITY, STATISTICAL CONCEPTS</h2>

<table>
<thead>
<tr>
<th align="right">Week of <sup>9</sup>&frasl;<sub>10</sub></th>
<th align="left"><a href="https://github.com/kmsaumcis/mcis6123_fa18/tree/master/lecture_notes">Lecture Notes</a></th>
</tr>
</thead>

<tbody>
<tr>
<td align="right"><strong>Content</strong></td>
<td align="left">introduction to comparing data using common metrics; introductory concepts in disorder; introductory statistical concepts; intuitions over data dimensionality and common reduction techniques</td>
</tr>

<tr>
<td align="right"><strong>Expected<br/>Outcomes</strong></td>
<td align="left">&#8226; identify common distance metrics and their appropriate contexts<br/>&#8226; understand similarity (and dissimilarity) in data<br/>&#8226; develop intuitions of statistical concepts in correlation, distributions and expect value<br/>&#8226; understand dimensionality reduction via PCA<br/>&#8226; utilize and apply basic statistical tools in Python (Pandas/Numpy)<br/></td>
</tr>

<tr>
<td align="right"><strong>Readings &amp;<br/>Supplemental</strong></td>
<td align="left"><strong>REQUIRED</strong><br/>&raquo; 2014. Zaki, Mohammed J and Meira Jr, Wagner; <a href="http://www.dataminingbook.info/pmwiki.php"><em>Data mining and analysis: fundamental concepts and algorithms</em></a>. &#8594; <strong>ch.7</strong><br/>&raquo; 2014. Leskovec, Jure and Rajaraman, Anand and Ullman, Jeffrey David; <a href="http://www.mmds.org/"><em>Mining of massive datasets</em></a>. &#8594; <strong>ch.11</strong><br/>&raquo; 2011. Han, Jiawei and Pei, Jian and Kamber, Micheline; <a href="https://ia800300.us.archive.org/5/items/DataMiningConceptAndTechniques2ndEdition/Data.Mining.Concepts.and.Techniques.2nd.Ed-1558609016.pdf"><em>Data mining: concepts and techniques</em></a>. &#8594; <strong>ch.2.5, ch.10.4.2</strong><br/>&raquo; 2017. VanderPlas, Jake; <a href="https://github.com/jakevdp/PythonDataScienceHandbook"><em>Python Data Science Handbook</em></a>. &#8594; <strong>ch.5.10 (In-depth Principal Components Analysis notebook)</strong><br/>&raquo; (website) -- 2017; <em>sklearn.neighbors.DistanceMetric class</em>: <a href="http://scikit-learn.org/stable/modules/generated/sklearn.neighbors.DistanceMetric.html">http://scikit-learn.org/stable/modules/generated/sklearn.neighbors.DistanceMetric.html</a>. &#8594; <strong>euclidean, cosine, jaccard</strong><br/><br/><strong>OPTIONAL</strong><br/>&rsaquo; 1997. Charles M. Grinstead, CM and Snell, JL; <a href="http://www.dartmouth.edu/~chance/teaching_aids/books_articles/probability_book/amsbook.mac.pdf"><em>Introduction to Probability</em></a>. &#8594; <strong>nice introductory resource to probability</strong><br/>&rsaquo; (website) -- 2017; <em>Distance computations (scipy.spatial.distance)</em>: <a href="https://docs.scipy.org/doc/scipy/reference/spatial.distance.html">https://docs.scipy.org/doc/scipy/reference/spatial.distance.html</a>. &#8594; <strong>cdist, euclidean, cosine, jaccard</strong><br/>&rsaquo; (O&rsquo;Reilly Data Show podcast) -- 07-06-2017; <em>A framework for building and evaluating data products</em>: <a href="https://www.oreilly.com/ideas/a-framework-for-building-and-evaluating-data-products">https://www.oreilly.com/ideas/a-framework-for-building-and-evaluating-data-products</a>. &#8594; <strong>listen to the entire interview</strong><br/></td>
</tr>

<tr>
<td align="right"><strong>Homework</strong></td>
<td align="left">&ndash;</td>
</tr>
</tbody>
</table>

<h2 id="lecture-4-association-rule-mining-pattern-mining">LECTURE 4: ASSOCIATION RULE MINING, PATTERN MINING</h2>

<table>
<thead>
<tr>
<th align="right">Week of <sup>9</sup>&frasl;<sub>17</sub></th>
<th align="left"><a href="https://github.com/kmsaumcis/mcis6273_f18_datamining/tree/master/lecture_notes">Lecture Notes</a></th>
</tr>
</thead>

<tbody>
<tr>
<td align="right"><strong>Content</strong></td>
<td align="left">introduction to concepts for rule and pattern mining; introdcution to apriori algorithm for frequent patterns; motivating the market basket analysis context for pattern mining; exploring addition contexts</td>
</tr>

<tr>
<td align="right"><strong>Expected<br/>Outcomes</strong></td>
<td align="left">&#8226; understand concepts behind frequent patterns<br/>&#8226; understand association rule mining, apriori algorithm, FP-growth<br/>&#8226; apply and compute basic patterns by hand<br/>&#8226; identify the contexts for applying pattern mining<br/></td>
</tr>

<tr>
<td align="right"><strong>Readings &amp;<br/>Supplemental</strong></td>
<td align="left"><strong>REQUIRED</strong><br/>&raquo; 2014. Zaki, Mohammed J and Meira Jr, Wagner; <a href="http://www.dataminingbook.info/pmwiki.php"><em>Data mining and analysis: fundamental concepts and algorithms</em></a>. &#8594; <strong>ch.8</strong><br/>&raquo; 2011. Han, Jiawei and Pei, Jian and Kamber, Micheline; <a href="https://ia800300.us.archive.org/5/items/DataMiningConceptAndTechniques2ndEdition/Data.Mining.Concepts.and.Techniques.2nd.Ed-1558609016.pdf"><em>Data mining: concepts and techniques</em></a>. &#8594; <strong>ch.5</strong><br/><br/><strong>OPTIONAL</strong><br/>&rsaquo; (PartiallyDerivative.com podcast) -- 06-13-2017; <em>The Secret Life Of A Data Scientist</em>: <a href="http://partiallyderivative.com/podcast/2017/06/13/the-secret-life-of-a-data-scientist">http://partiallyderivative.com/podcast/2017/06/13/the-secret-life-of-a-data-scientist</a>. &#8594; <strong>listen to the entire podcast</strong><br/></td>
</tr>

<tr>
<td align="right"><strong>Homework</strong></td>
<td align="left"><strong>DUE:</strong> Monday, <sup>10</sup>&frasl;<sub>1</sub> - midnight<br/>Please see the  Blackboard/<a href="https://github.com/kmsaumcis/mcis6273_f18_datamining/homework">Github repo</a> for what to turn in.</td>
</tr>
</tbody>
</table>

<h2 id="lecture-5-unsupervised-techniques-introduction-to-clustering">LECTURE 5: UNSUPERVISED TECHNIQUES / INTRODUCTION TO CLUSTERING</h2>

<table>
<thead>
<tr>
<th align="right">Week of <sup>9</sup>&frasl;<sub>24</sub></th>
<th align="left"><a href="https://github.com/kmsaumcis/mcis6273_f18_datamining/tree/master/lecture_notes">Lecture Notes</a></th>
</tr>
</thead>

<tbody>
<tr>
<td align="right"><strong>Content</strong></td>
<td align="left">introduction to cluster analysis and motivations; introduction to unsupervised clustering algorithms; partitioning (k-means, k-mediods); hierarchical agglomerative methods; model-based (expectation-maximization) neural networks (SOM self-organizing maps); visualing with voronoi diagrams</td>
</tr>

<tr>
<td align="right"><strong>Expected<br/>Outcomes</strong></td>
<td align="left">&#8226; exposure to unsupervised clustering methods, k-Means<br/>&#8226; introduction to key clustering algorithms<br/>&#8226; distinguish between partition and model-based algorithms<br/></td>
</tr>

<tr>
<td align="right"><strong>Readings &amp;<br/>Supplemental</strong></td>
<td align="left"><strong>REQUIRED</strong><br/>&raquo; 2014. Zaki, Mohammed J and Meira Jr, Wagner; <a href="http://www.dataminingbook.info/pmwiki.php"><em>Data mining and analysis: fundamental concepts and algorithms</em></a>. &#8594; <strong>ch.13, ch.14, ch.15, ch.17</strong><br/>&raquo; 2011. Han, Jiawei and Pei, Jian and Kamber, Micheline; <a href="https://ia800300.us.archive.org/5/items/DataMiningConceptAndTechniques2ndEdition/Data.Mining.Concepts.and.Techniques.2nd.Ed-1558609016.pdf"><em>Data mining: concepts and techniques</em></a>. &#8594; <strong>ch.7</strong><br/>&raquo; (website) -- 2015; <em>Basic Clustering with k-Means</em>: <a href="https://nbviewer.jupyter.org/github/tmbdev/teaching-mmir/blob/master/30-kmeans.ipynb">https://nbviewer.jupyter.org/github/tmbdev/teaching-mmir/blob/master/30-kmeans.ipynb</a>. &#8594; <strong>Familiarize yourself with the notebook.</strong><br/><br/><strong>OPTIONAL</strong><br/>&rsaquo; (LinearDigressions.com podcast) -- 04-16-2017; <em>Education Analytics</em>: <a href="http://lineardigressions.com/episodes/2017/4/16/education-analytics">http://lineardigressions.com/episodes/2017/4/16/education-analytics</a>. &#8594; <strong>listen to the entire podcast</strong><br/>&rsaquo; (website) -- --; <em>Programatically understanding Expectation Maximization</em>: <a href="https://nipunbatra.github.io/blog/2014/em.html">https://nipunbatra.github.io/blog/2014/em.html</a>. &#8594; <strong>read this practical explanation (with Python code) of the EM algorithm</strong><br/>&rsaquo; (website) -- --; <em>Agglomerative Clustering Essentials</em>: <a href="http://www.sthda.com/english/articles/28-hierarchical-clustering-essentials/90-agglomerative-clustering-essentials/">http://www.sthda.com/english/articles/28-hierarchical-clustering-essentials/90-agglomerative-clustering-essentials/</a>. &#8594; <strong>a nice introduction to agglomerative clustering with examples in R</strong><br/>&rsaquo; (website) -- --; *Introduction to Information Retrieval</td>
</tr>

<tr>
<td align="right"><strong>Homework</strong></td>
<td align="left"><strong>DUE:</strong> Monday, <sup>10</sup>&frasl;<sub>22</sub> - midnight<br/>Please see the  Blackboard/<a href="https://github.com/kmsaumcis/mcis6273_f18_datamining/homework">Github repo</a> for what to turn in.</td>
</tr>
</tbody>
</table>

<h2 id="lecture-6-unsupervised-techniques-more-clustering">LECTURE 6: UNSUPERVISED TECHNIQUES / MORE CLUSTERING</h2>

<table>
<thead>
<tr>
<th align="right">Week of <sup>10</sup>&frasl;<sub>1</sub></th>
<th align="left"><a href="https://github.com/kmsaumcis/mcis6273_f18_datamining/tree/master/lecture_notes">Lecture Notes</a></th>
</tr>
</thead>

<tbody>
<tr>
<td align="right"><strong>Content</strong></td>
<td align="left">continued clustering, hierachical algorithms (agglomorative), introduction to density-based algorithms (DBSCAN)</td>
</tr>

<tr>
<td align="right"><strong>Expected<br/>Outcomes</strong></td>
<td align="left">&#8226; understand hierarchical and density-based algorithms<br/>&#8226; develop intuitions for choosing algorithms in various contexts<br/>&#8226; utilize algorithms on read-world data<br/></td>
</tr>

<tr>
<td align="right"><strong>Readings &amp;<br/>Supplemental</strong></td>
<td align="left">No assigned readings. Please complete readings from previous week if not current.</td>
</tr>

<tr>
<td align="right"><strong>Homework</strong></td>
<td align="left">&ndash;</td>
</tr>
</tbody>
</table>

<h2 id="lecture-7-supervised-techniques-classification-and-prediction">LECTURE 7: SUPERVISED TECHNIQUES / CLASSIFICATION AND PREDICTION</h2>

<table>
<thead>
<tr>
<th align="right">Week of <sup>10</sup>&frasl;<sub>8</sub></th>
<th align="left"><a href="https://github.com/kmsaumcis/mcis6273_f18_datamining/tree/master/lecture_notes">Lecture Notes</a></th>
</tr>
</thead>

<tbody>
<tr>
<td align="right"><strong>Content</strong></td>
<td align="left">classification and prediction; understanding decision trees, concepts and theory; probabilistic approaches to classification - na&iuml;ve bayes; introduction to bayesian belief networks</td>
</tr>

<tr>
<td align="right"><strong>Expected<br/>Outcomes</strong></td>
<td align="left">&#8226; understand and explain decision trees<br/>&#8226; develop probabilistic models of classification using na&iuml;ve Bayes<br/>&#8226; identify BBNs and their application context<br/>&#8226; utilize na&iuml;ve Bayes in real-world applications<br/></td>
</tr>

<tr>
<td align="right"><strong>Readings &amp;<br/>Supplemental</strong></td>
<td align="left"><strong>REQUIRED</strong><br/>&raquo; 2014. Zaki, Mohammed J and Meira Jr, Wagner; <a href="http://www.dataminingbook.info/pmwiki.php"><em>Data mining and analysis: fundamental concepts and algorithms</em></a>. &#8594; <strong>ch.18, ch.19</strong><br/>&raquo; 2011. Han, Jiawei and Pei, Jian and Kamber, Micheline; <a href="https://ia800300.us.archive.org/5/items/DataMiningConceptAndTechniques2ndEdition/Data.Mining.Concepts.and.Techniques.2nd.Ed-1558609016.pdf"><em>Data mining: concepts and techniques</em></a>. &#8594; <strong>ch.6.3, ch.6.4</strong><br/><br/><strong>OPTIONAL</strong><br/>&rsaquo; (DataSkeptic.com podcast) -- 08-04-2017; <em>MINI: Bayesian Belief Networks</em>: <a href="https://dataskeptic.com/blog/episodes/2017/bayesian-belief-networks">https://dataskeptic.com/blog/episodes/2017/bayesian-belief-networks</a>. &#8594; <strong>explore this light discussion of BBNs</strong><br/>&rsaquo; 2012. Barber, D.; <a href="http://web4.cs.ucl.ac.uk/staff/D.Barber/textbook/020217.pdf"><em>Bayesian Reasoning and Machine Learning</em></a>. &#8594; <strong>explore ch.3 for in a deeper theoretical treatment of BBNs</strong><br/></td>
</tr>

<tr>
<td align="right"><strong>Homework</strong></td>
<td align="left"><strong>DUE:</strong> Monday, <sup>11</sup>&frasl;<sub>5</sub> - midnight<br/>Please see the  Blackboard/<a href="https://github.com/kmsaumcis/mcis6273_f18_datamining/homework">Github repo</a> for what to turn in.</td>
</tr>
</tbody>
</table>

<h2 id="lecture-8-supervised-techniques-classification-and-prediction">LECTURE 8: SUPERVISED TECHNIQUES / CLASSIFICATION AND PREDICTION</h2>

<table>
<thead>
<tr>
<th align="right">Week of <sup>10</sup>&frasl;<sub>15</sub></th>
<th align="left"><a href="https://github.com/kmsaumcis/mcis6273_f18_datamining/tree/master/lecture_notes">Lecture Notes</a></th>
</tr>
</thead>

<tbody>
<tr>
<td align="right"><strong>Content</strong></td>
<td align="left">linear regression  models for prediction; logistic regression models for prediction; introduction to generalized linear models</td>
</tr>

<tr>
<td align="right"><strong>Expected<br/>Outcomes</strong></td>
<td align="left">&#8226; understand and develop linear regression models<br/>&#8226; understand and interpret logistic regression models<br/>&#8226; exposure to generalized linear models<br/></td>
</tr>

<tr>
<td align="right"><strong>Readings &amp;<br/>Supplemental</strong></td>
<td align="left"><strong>REQUIRED</strong><br/>&raquo; 2014. Zaki, Mohammed J and Meira Jr, Wagner; <a href="http://www.dataminingbook.info/pmwiki.php"><em>Data mining and analysis: fundamental concepts and algorithms</em></a>. &#8594; <strong>ch.20</strong><br/>&raquo; 2011. Han, Jiawei and Pei, Jian and Kamber, Micheline; <a href="https://ia800300.us.archive.org/5/items/DataMiningConceptAndTechniques2ndEdition/Data.Mining.Concepts.and.Techniques.2nd.Ed-1558609016.pdf"><em>Data mining: concepts and techniques</em></a>. &#8594; <strong>ch.6.11</strong><br/><br/><strong>OPTIONAL</strong><br/>&rsaquo; (DataSkeptic.com podcast) -- 01-27-2017; <em>MINI: Logistic Regression on Audio Data</em>: <a href="https://dataskeptic.com/blog/episodes/2017/logistic-regression-on-audio-data">https://dataskeptic.com/blog/episodes/2017/logistic-regression-on-audio-data</a>. &#8594; <strong>listen to the entire podcast</strong><br/>&rsaquo; (website) -- --; <em>Building a logistic regression classifier from the ground up</em>: <a href="http://inmachineswetrust.com/posts/building-logistic-regression/">http://inmachineswetrust.com/posts/building-logistic-regression/</a>. &#8594; <strong>this is a nice explanation (and code) in Python</strong><br/></td>
</tr>

<tr>
<td align="right"><strong>Homework</strong></td>
<td align="left">&ndash;</td>
</tr>
</tbody>
</table>

<h2 id="lecture-9-supervised-techniques-classification-and-model-evaluation">LECTURE 9: SUPERVISED TECHNIQUES / CLASSIFICATION AND MODEL EVALUATION</h2>

<table>
<thead>
<tr>
<th align="right">Week of <sup>10</sup>&frasl;<sub>22</sub></th>
<th align="left"><a href="https://github.com/kmsaumcis/mcis6273_f18_datamining/tree/master/lecture_notes">Lecture Notes</a></th>
</tr>
</thead>

<tbody>
<tr>
<td align="right"><strong>Content</strong></td>
<td align="left">support vector machines; neural networks and the basic NN model and its relation to learning algorithms; evaluating models and applying techniques to model validation</td>
</tr>

<tr>
<td align="right"><strong>Expected<br/>Outcomes</strong></td>
<td align="left">&#8226; understand support vector machines and their strengths<br/>&#8226; understand neural networks, their basic theory and application<br/>&#8226; identify and develop intutition around model evaluation and validation<br/></td>
</tr>

<tr>
<td align="right"><strong>Readings &amp;<br/>Supplemental</strong></td>
<td align="left"><strong>REQUIRED</strong><br/>&raquo; 2014. Zaki, Mohammed J and Meira Jr, Wagner; <a href="http://www.dataminingbook.info/pmwiki.php"><em>Data mining and analysis: fundamental concepts and algorithms</em></a>. &#8594; <strong>ch.21</strong><br/>&raquo; 2011. Han, Jiawei and Pei, Jian and Kamber, Micheline; <a href="https://ia800300.us.archive.org/5/items/DataMiningConceptAndTechniques2ndEdition/Data.Mining.Concepts.and.Techniques.2nd.Ed-1558609016.pdf"><em>Data mining: concepts and techniques</em></a>. &#8594; <strong>ch.6.6, ch.6.7</strong><br/><br/><strong>OPTIONAL</strong><br/>&rsaquo; (DataSkeptic.com podcast) -- 05-27-2017; <em>Data Science at eHarmony</em>: <a href="https://dataskeptic.com/blog/episodes/2016/data-science-at-eharmony">https://dataskeptic.com/blog/episodes/2016/data-science-at-eharmony</a>. &#8594; <strong>listen to the entire podcast</strong><br/></td>
</tr>

<tr>
<td align="right"><strong>Homework</strong></td>
<td align="left"><strong>DUE:</strong> Monday, <sup>11</sup>&frasl;<sub>26</sub> - midnight<br/>Please see the  Blackboard/<a href="https://github.com/kmsaumcis/mcis6273_f18_datamining/homework">Github repo</a> for what to turn in.</td>
</tr>
</tbody>
</table>

<h2 id="lecture-10-ensemble-methods">LECTURE 10: ENSEMBLE METHODS</h2>

<table>
<thead>
<tr>
<th align="right">Week of <sup>10</sup>&frasl;<sub>29</sub></th>
<th align="left"><a href="https://github.com/kmsaumcis/mcis6273_f18_datamining/tree/master/lecture_notes">Lecture Notes</a></th>
</tr>
</thead>

<tbody>
<tr>
<td align="right"><strong>Content</strong></td>
<td align="left">ensemble methods; introduction to boosting, bagging, random forests and related methods</td>
</tr>

<tr>
<td align="right"><strong>Expected<br/>Outcomes</strong></td>
<td align="left">&#8226; understand and identify the need for ensembles<br/>&#8226; identify and develop intutition around ensemble model evaluation and validation<br/></td>
</tr>

<tr>
<td align="right"><strong>Readings &amp;<br/>Supplemental</strong></td>
<td align="left"><strong>REQUIRED</strong><br/>&raquo; 2014. Zaki, Mohammed J and Meira Jr, Wagner; <a href="http://www.dataminingbook.info/pmwiki.php"><em>Data mining and analysis: fundamental concepts and algorithms</em></a>. &#8594; <strong>ch.22</strong><br/>&raquo; 2011. Han, Jiawei and Pei, Jian and Kamber, Micheline; <a href="https://ia800300.us.archive.org/5/items/DataMiningConceptAndTechniques2ndEdition/Data.Mining.Concepts.and.Techniques.2nd.Ed-1558609016.pdf"><em>Data mining: concepts and techniques</em></a>. &#8594; <strong>ch.6.12, ch.6.13, ch.6.14, ch.6.15</strong><br/></td>
</tr>

<tr>
<td align="right"><strong>Homework</strong></td>
<td align="left">&ndash;</td>
</tr>
</tbody>
</table>

<h2 id="lecture-11-data-visualization-introductory-concepts">LECTURE 11: DATA VISUALIZATION: INTRODUCTORY CONCEPTS</h2>

<table>
<thead>
<tr>
<th align="right">Week of <sup>11</sup>&frasl;<sub>5</sub></th>
<th align="left"><a href="https://github.com/kmsaumcis/mcis6273_f18_datamining/tree/master/lecture_notes">Lecture Notes</a></th>
</tr>
</thead>

<tbody>
<tr>
<td align="right"><strong>Content</strong></td>
<td align="left">introduction to data visualization; building data narratives</td>
</tr>

<tr>
<td align="right"><strong>Expected<br/>Outcomes</strong></td>
<td align="left">&#8226; understand basic concepts in visualization<br/>&#8226; familiarity with Python libraries for visualizing data<br/></td>
</tr>

<tr>
<td align="right"><strong>Readings &amp;<br/>Supplemental</strong></td>
<td align="left"><strong>REQUIRED</strong><br/>&raquo; 2015. Knaflic, Cole Nussbaumer; <a href="http://www.storytellingwithdata.com/book/"><em>Storytelling with data: A data visualization guide for business professionals</em></a>. &#8594; <strong>ch.8</strong><br/>&raquo; (website) -- 2017; <em>D3.js: Data-Driven Documents</em>: <a href="http://d3js.org">http://d3js.org</a>. &#8594; <strong>familiarize yourself with some of the visualizations and capabilities of D3.js</strong><br/><br/><strong>OPTIONAL</strong><br/>&rsaquo; 2014. B\&ldquo;orner, Katy and Polley, David E; <a href="https://mitpress.mit.edu/books/visual-insights"><em>Visual insights: A practical guide to making sense of data</em></a>. &#8594; <strong>ch.5</strong><br/>&rsaquo; (website) -- 2017; <em>Analyzing Scrabble Games</em>: <a href="http://rpubs.com/jalapic/scrabblr">http://rpubs.com/jalapic/scrabblr</a>. &#8594; <strong>This is a very interesting exploration in analysis and visualization.</strong><br/>&rsaquo; (website) -- 2017; <em>World Population Growth</em>: <a href="https://ourworldindata.org/world-population-growth/">https://ourworldindata.org/world-population-growth/</a>. &#8594; <strong>explore some of the data and visualizations</strong><br/>&rsaquo; (website) -- 2017; <em>RAWGraphs: The missing link between spreadsheets and data visualization</em>: <a href="http://rawgraphs.io/">http://rawgraphs.io/</a>. &#8594; <strong>explore this site and its galleries</strong><br/>&rsaquo; (website) -- 2016; <em>Rio 2016 Medals Race: An analysis of the 2016 Olympic Medals</em>: <a href="http://timesofoman.com/extra/rio_2016_medal_tally/index.html">http://timesofoman.com/extra/rio_2016_medal_tally/index.html</a>. &#8594; <strong>explore this visualization</strong><br/>&rsaquo; (website) -- --; <em>The Ultimate Python Seaborn Tutorial: Gotta Catch ‘Em All</em>: <a href="https://elitedatascience.com/python-seaborn-tutorial">https://elitedatascience.com/python-seaborn-tutorial</a>. &#8594; <strong>explore this nice tutorial on Seaborn</strong><br/>&rsaquo; (website) -- --; <em>Principles For Effective Data Visualization</em>: <a href="https://blog.zingchart.com/2014/09/08/principles-effective-data-visualization/">https://blog.zingchart.com/2014/09/08/principles-effective-data-visualization/</a>. &#8594; <strong>a CONVINCE method to visualization concepts</strong><br/>&rsaquo; (website) -- --; *Towards Data Science Blog</td>
</tr>

<tr>
<td align="right"><strong>Homework</strong></td>
<td align="left">&ndash;</td>
</tr>
</tbody>
</table>

<h2 id="lecture-12-introduction-to-social-mining">LECTURE 12: INTRODUCTION TO SOCIAL MINING</h2>

<table>
<thead>
<tr>
<th align="right">Week of <sup>11</sup>&frasl;<sub>12</sub></th>
<th align="left"><a href="https://github.com/kmsaumcis/mcis6273_f18_datamining/tree/master/lecture_notes">Lecture Notes</a></th>
</tr>
</thead>

<tbody>
<tr>
<td align="right"><strong>Content</strong></td>
<td align="left">introduction to social mining; introduction to recommendation systems, collaborative and content-based filtering</td>
</tr>

<tr>
<td align="right"><strong>Expected<br/>Outcomes</strong></td>
<td align="left">&#8226; understand core social mining algorithms<br/>&#8226; understand concepts in network analysis<br/>&#8226; understand core recommender system concepts<br/></td>
</tr>

<tr>
<td align="right"><strong>Readings &amp;<br/>Supplemental</strong></td>
<td align="left"><strong>REQUIRED</strong><br/>&raquo; 2014. Leskovec, Jure and Rajaraman, Anand and Ullman, Jeffrey David; <a href="http://www.mmds.org/"><em>Mining of massive datasets</em></a>. &#8594; <strong>ch.10</strong><br/>&raquo; 2015. Grus, Joel; <a href="http://shop.oreilly.com/product/0636920033400.do"><em>Data science from scratch: First principles with Python</em></a>. &#8594; <strong>ch.22</strong><br/><br/><strong>OPTIONAL</strong><br/>&rsaquo; 2014. Leskovec, Jure and Rajaraman, Anand and Ullman, Jeffrey David; <a href="http://www.mmds.org/"><em>Mining of massive datasets</em></a>. &#8594; <strong>ch.9</strong><br/>&rsaquo; 2014. B\&ldquo;orner, Katy and Polley, David E; <a href="https://mitpress.mit.edu/books/visual-insights"><em>Visual insights: A practical guide to making sense of data</em></a>. &#8594; <strong>ch.5</strong><br/></td>
</tr>

<tr>
<td align="right"><strong>Homework</strong></td>
<td align="left">&ndash;</td>
</tr>
</tbody>
</table>

<h2 id="lecture-13-introduction-to-text-mining">LECTURE 13: INTRODUCTION TO TEXT MINING</h2>

<table>
<thead>
<tr>
<th align="right">Week of <sup>11</sup>&frasl;<sub>19</sub></th>
<th align="left"><a href="https://github.com/kmsaumcis/mcis6273_f18_datamining/tree/master/lecture_notes">Lecture Notes</a></th>
</tr>
</thead>

<tbody>
<tr>
<td align="right"><strong>Content</strong></td>
<td align="left">introduction to text mining; concepts in document preparation pipeline (tokenizing, stemming, etc.); TFIDF, cosine similarity; corpus selection</td>
</tr>

<tr>
<td align="right"><strong>Expected<br/>Outcomes</strong></td>
<td align="left">&#8226; understand introductory concepts in text mining and information retrieval<br/>&#8226; understand document preparation tools<br/>&#8226; apply basic concepts to real-world data<br/></td>
</tr>

<tr>
<td align="right"><strong>Readings &amp;<br/>Supplemental</strong></td>
<td align="left"><strong>REQUIRED</strong><br/>&raquo; 2011. Han, Jiawei and Pei, Jian and Kamber, Micheline; <a href="https://ia800300.us.archive.org/5/items/DataMiningConceptAndTechniques2ndEdition/Data.Mining.Concepts.and.Techniques.2nd.Ed-1558609016.pdf"><em>Data mining: concepts and techniques</em></a>. &#8594; <strong>ch.10.4</strong><br/>&raquo; 2008. Manning, Christopher D and Raghavan, Prabhakar and Sch\&ldquo;utze, Hinrich; <a href="https://nlp.stanford.edu/IR-book/information-retrieval-book.html"><em>Introduction to information retrieval</em></a>. &#8594; <strong>ch.6</strong><br/><br/><strong>OPTIONAL</strong><br/>&rsaquo; 2008. Manning, Christopher D and Raghavan, Prabhakar and Sch\&ldquo;utze, Hinrich; <a href="https://nlp.stanford.edu/IR-book/information-retrieval-book.html"><em>Introduction to information retrieval</em></a>. &#8594; <strong>ch.13</strong><br/>&rsaquo; (O&rsquo;Reilly Data Show podcast) -- 07-06-2017; <em>Language understanding remains one of AI’s grand challenges</em>: <a href="https://www.oreilly.com/ideas/language-understanding-remains-one-of-ais-grand-challenges">https://www.oreilly.com/ideas/language-understanding-remains-one-of-ais-grand-challenges</a>. &#8594; <strong>listen to the entire interview</strong><br/>&rsaquo; (LinearDigressions.com podcast) -- 04-30-2017; <em>Word2Vec</em>: <a href="http://lineardigressions.com/episodes/2017/4/30/word2vec">http://lineardigressions.com/episodes/2017/4/30/word2vec</a>. &#8594; <strong>listen to the entire podcast</strong><br/></td>
</tr>

<tr>
<td align="right"><strong>Homework</strong></td>
<td align="left">&ndash;</td>
</tr>
</tbody>
</table>

<h2 id="lecture-14-open-data-ethics-in-data-mining-the-future-of-data-science">LECTURE 14: OPEN DATA, ETHICS IN DATA MINING, THE FUTURE OF DATA SCIENCE</h2>

<table>
<thead>
<tr>
<th align="right">Week of <sup>11</sup>&frasl;<sub>27</sub></th>
<th align="left"><a href="https://github.com/kmsaumcis/mcis6273_f18_datamining/tree/master/lecture_notes">Lecture Notes</a></th>
</tr>
</thead>

<tbody>
<tr>
<td align="right"><strong>Content</strong></td>
<td align="left">open data portals, APIs, tools and technologies; ethics in data mining; anonymization, privacy and data considerations; data science and the future</td>
</tr>

<tr>
<td align="right"><strong>Expected<br/>Outcomes</strong></td>
<td align="left">&#8226; exposure to open data portals and open data technologies<br/>&#8226; exposure to open APIs and tools for open data access<br/>&#8226; understand data mining ethics and why ethics (and privacy) are critically important<br/>&#8226; the future to data science, analytics and intelligent systems built on big data<br/></td>
</tr>

<tr>
<td align="right"><strong>Readings &amp;<br/>Supplemental</strong></td>
<td align="left"><strong>REQUIRED</strong><br/>&raquo; (DataStori.es podcast) -- 05-18-2016; <em>74 - Data Ethics and Privacy with Eleanor Saitta</em>: <a href="http://datastori.es/74-data-ethics-and-privacy-with-eleanor-saitta/">http://datastori.es/74-data-ethics-and-privacy-with-eleanor-saitta/</a>. &#8594; <strong>listen to the entire podcast</strong><br/>&raquo; (website) -- 2017; <em>ProgrammableWeb.com: The Journal of the API Economy</em>: <a href="https://www.programmableweb.com/">https://www.programmableweb.com/</a>. &#8594; <strong>familiarize yourself with this site and some APIs</strong><br/><br/><strong>OPTIONAL</strong><br/>&rsaquo; (LinearDigressions.com podcast) -- 08-13-2017; <em>Curing Cancer with Machine Learning is Super Hard</em>: <a href="http://lineardigressions.com/episodes/2017/8/13/curing-cancer-with-machine-learning-is-super-hard">http://lineardigressions.com/episodes/2017/8/13/curing-cancer-with-machine-learning-is-super-hard</a>. &#8594; <strong>listen to the entire podcast</strong><br/></td>
</tr>

<tr>
<td align="right"><strong>Homework</strong></td>
<td align="left">&ndash;</td>
</tr>
</tbody>
</table>

<h2 id="resources">RESOURCES</h2>

<ol>
<li>2014. Zaki, Mohammed J and Meira Jr, Wagner; <a href="http://www.dataminingbook.info/pmwiki.php"><em>Data mining and analysis: fundamental concepts and algorithms</em></a>.<br/></li>
<li>2014. Leskovec, Jure and Rajaraman, Anand and Ullman, Jeffrey David; <a href="http://www.mmds.org/"><em>Mining of massive datasets</em></a>.<br/></li>
<li>1997. Charles M. Grinstead, CM and Snell, JL; <a href="http://www.dartmouth.edu/~chance/teaching_aids/books_articles/probability_book/amsbook.mac.pdf"><em>Introduction to Probability</em></a>.<br/></li>
<li>2011. Yau, Nathan; <a href="http://book.flowingdata.com/"><em>Visualize this: the FlowingData guide to design, visualization, and statistics</em></a>.<br/></li>
<li>2014. B{\&ldquo;o}rner, Katy and Polley, David E; <a href="https://mitpress.mit.edu/books/visual-insights"><em>Visual insights: A practical guide to making sense of data</em></a>.<br/></li>
<li>2012. Downey, Allen; <a href="http://www.greenteapress.com/thinkpython/thinkpython.html"><em>Think Python</em></a>.<br/></li>
<li>2012. Conway, Drew and White, John; <a href="http://shop.oreilly.com/product/0636920018483.do"><em>Machine learning for hackers</em></a>.<br/></li>
<li>2015. Grus, Joel; <a href="http://shop.oreilly.com/product/0636920033400.do"><em>Data science from scratch: First principles with Python</em></a>.<br/></li>
<li>(website) -- 2017; <em>The Periodic Table of Data Science</em>: <a href="https://www.datacamp.com/community/blog/data-science-periodic-table#gs.TF297Gsm">https://www.datacamp.com/community/blog/data-science-periodic-table#gs.TF297Gsm</a>.<br/></li>
<li>2011. Han, Jiawei and Pei, Jian and Kamber, Micheline; <a href="https://ia800300.us.archive.org/5/items/DataMiningConceptAndTechniques2ndEdition/Data.Mining.Concepts.and.Techniques.2nd.Ed-1558609016.pdf"><em>Data mining: concepts and techniques</em></a>.<br/></li>
<li>2012. McKinney, Wes; <a href="https://github.com/wesm/pydata-book"><em>Python for data analysis: Data wrangling with Pandas, NumPy, and IPython</em></a>.<br/></li>
<li>2008. Manning, Christopher D and Raghavan, Prabhakar and Sch{\&ldquo;u}tze, Hinrich; <a href="https://nlp.stanford.edu/IR-book/information-retrieval-book.html"><em>Introduction to information retrieval</em></a>.<br/></li>
<li>2015. Knaflic, Cole Nussbaumer; <a href="http://www.storytellingwithdata.com/book/"><em>Storytelling with data: A data visualization guide for business professionals</em></a>.<br/></li>
<li>2016. Rose, Doug; <a href="http://www.apress.com/us/book/9781484222522"><em>Data Science: Create Teams That Ask the Right Questions and Deliver Real Value</em></a>.<br/></li>
<li>(website) -- 2013; <em>Mining the Social Web: Data Mining Facebook, Twitter, LinkedIn, Google+, GitHub, and More</em>: <a href="https://github.com/ptwobrussell/Mining-the-Social-Web-2nd-Edition/">https://github.com/ptwobrussell/Mining-the-Social-Web-2nd-Edition/</a>.<br/></li>
<li>2017. Wexler, Steve and Shaffer, Jeffrey and Cotgreave, Andy; <a href="http://bigbookofdashboards.com"><em>The Big Book of Dashboards: Visualizing Your Data Using Real-World Business Scenarios</em></a>.<br/></li>
<li>2017. VanderPlas, Jake; <a href="https://github.com/jakevdp/PythonDataScienceHandbook"><em>Python Data Science Handbook</em></a>.<br/></li>
<li>(website) -- 2015; <em>Basic Clustering with k-Means</em>: <a href="https://nbviewer.jupyter.org/github/tmbdev/teaching-mmir/blob/master/30-kmeans.ipynb">https://nbviewer.jupyter.org/github/tmbdev/teaching-mmir/blob/master/30-kmeans.ipynb</a>.<br/></li>
<li>(website) -- 2017; <em>Distance computations (scipy.spatial.distance)</em>: <a href="https://docs.scipy.org/doc/scipy/reference/spatial.distance.html">https://docs.scipy.org/doc/scipy/reference/spatial.distance.html</a>.<br/></li>
<li>(website) -- 11-15-2016; <em>Jupyter Notebook Tutorial: The Definitive Guide</em>: <a href="https://www.datacamp.com/community/tutorials/tutorial-jupyter-notebook#gs.zExWvMw">https://www.datacamp.com/community/tutorials/tutorial-jupyter-notebook#gs.zExWvMw</a>.<br/></li>
<li>(website) -- 2017; <em>Pandas Cookbook</em>: <a href="https://github.com/jvns/pandas-cookbook">https://github.com/jvns/pandas-cookbook</a>.<br/></li>
<li>(website) -- 2017; <em>sklearn.neighbors.DistanceMetric class</em>: <a href="http://scikit-learn.org/stable/modules/generated/sklearn.neighbors.DistanceMetric.html">http://scikit-learn.org/stable/modules/generated/sklearn.neighbors.DistanceMetric.html</a>.<br/></li>
<li>({Michael Kennedy&rsquo;s Talk Python To Me} podcast) -- 11-28-2016; <em>Episode #90: Data Wrangling with Python</em>: <a href="http://talkpythontome.fm">http://talkpythontome.fm</a>.<br/></li>
<li>({O&rsquo;Reilly Data Show} podcast) -- 07-06-2017; <em>A framework for building and evaluating data products</em>: <a href="https://www.oreilly.com/ideas/a-framework-for-building-and-evaluating-data-products">https://www.oreilly.com/ideas/a-framework-for-building-and-evaluating-data-products</a>.<br/></li>
<li>({O&rsquo;Reilly Data Show} podcast) -- 07-06-2017; <em>Language understanding remains one of AI’s grand challenges</em>: <a href="https://www.oreilly.com/ideas/language-understanding-remains-one-of-ais-grand-challenges">https://www.oreilly.com/ideas/language-understanding-remains-one-of-ais-grand-challenges</a>.<br/></li>
<li>(PartiallyDerivative.com podcast) -- 06-13-2017; <em>The Secret Life Of A Data Scientist</em>: <a href="http://partiallyderivative.com/podcast/2017/06/13/the-secret-life-of-a-data-scientist">http://partiallyderivative.com/podcast/2017/06/13/the-secret-life-of-a-data-scientist</a>.<br/></li>
<li>(LinearDigressions.com podcast) -- 04-16-2017; <em>Education Analytics</em>: <a href="http://lineardigressions.com/episodes/2017/4/16/education-analytics">http://lineardigressions.com/episodes/2017/4/16/education-analytics</a>.<br/></li>
<li>(LinearDigressions.com podcast) -- 06-04-2017; <em>PageRank</em>: <a href="http://lineardigressions.com/episodes/2017/6/4/pagerank">http://lineardigressions.com/episodes/2017/6/4/pagerank</a>.<br/></li>
<li>(LinearDigressions.com podcast) -- 08-13-2017; <em>Curing Cancer with Machine Learning is Super Hard</em>: <a href="http://lineardigressions.com/episodes/2017/8/13/curing-cancer-with-machine-learning-is-super-hard">http://lineardigressions.com/episodes/2017/8/13/curing-cancer-with-machine-learning-is-super-hard</a>.<br/></li>
<li>(LinearDigressions.com podcast) -- 04-30-2017; <em>Word2Vec</em>: <a href="http://lineardigressions.com/episodes/2017/4/30/word2vec">http://lineardigressions.com/episodes/2017/4/30/word2vec</a>.<br/></li>
<li>(DataStori.es podcast) -- 05-18-2016; <em>74 - Data Ethics and Privacy with Eleanor Saitta</em>: <a href="http://datastori.es/74-data-ethics-and-privacy-with-eleanor-saitta/">http://datastori.es/74-data-ethics-and-privacy-with-eleanor-saitta/</a>.<br/></li>
<li>(website) -- 2017; <em>ProgrammableWeb.com: The Journal of the API Economy</em>: <a href="https://www.programmableweb.com/">https://www.programmableweb.com/</a>.<br/></li>
<li>(website) -- 2017; <em>Analyzing Scrabble Games</em>: <a href="http://rpubs.com/jalapic/scrabblr">http://rpubs.com/jalapic/scrabblr</a>.<br/></li>
<li>(website) -- 2017; <em>GSS Data Explorer</em>: <a href="https://gssdataexplorer.norc.org/">https://gssdataexplorer.norc.org/</a>.<br/></li>
<li>(website) -- 2017; <em>World Population Growth</em>: <a href="https://ourworldindata.org/world-population-growth/">https://ourworldindata.org/world-population-growth/</a>.<br/></li>
<li>(website) -- 2017; <em>RAWGraphs: The missing link between spreadsheets and data visualization</em>: <a href="http://rawgraphs.io/">http://rawgraphs.io/</a>.<br/></li>
<li>(website) -- 2016; <em>Rio 2016 Medals Race: An analysis of the 2016 Olympic Medals</em>: <a href="http://timesofoman.com/extra/rio_2016_medal_tally/index.html">http://timesofoman.com/extra/rio_2016_medal_tally/index.html</a>.<br/></li>
<li>(website) -- 2017; <em>D3.js: Data-Driven Documents</em>: <a href="http://d3js.org">http://d3js.org</a>.<br/></li>
<li>(DataSkeptic.com podcast) -- 08-04-2017; <em>MINI: Bayesian Belief Networks</em>: <a href="https://dataskeptic.com/blog/episodes/2017/bayesian-belief-networks">https://dataskeptic.com/blog/episodes/2017/bayesian-belief-networks</a>.<br/></li>
<li>(DataSkeptic.com podcast) -- 01-27-2017; <em>MINI: Logistic Regression on Audio Data</em>: <a href="https://dataskeptic.com/blog/episodes/2017/logistic-regression-on-audio-data">https://dataskeptic.com/blog/episodes/2017/logistic-regression-on-audio-data</a>.<br/></li>
<li>(DataSkeptic.com podcast) -- 05-27-2017; <em>Data Science at eHarmony</em>: <a href="https://dataskeptic.com/blog/episodes/2016/data-science-at-eharmony">https://dataskeptic.com/blog/episodes/2016/data-science-at-eharmony</a>.<br/></li>
<li>(website) -- --; <em>Binning Data In Pandas</em>: <a href="https://chrisalbon.com/python/pandas_binning_data.html">https://chrisalbon.com/python/pandas_binning_data.html</a>.<br/></li>
<li>(website) -- --; <em>Programatically understanding Expectation Maximization</em>: <a href="https://nipunbatra.github.io/blog/2014/em.html">https://nipunbatra.github.io/blog/2014/em.html</a>.<br/></li>
<li>(website) -- --; <em>The Ultimate Python Seaborn Tutorial: Gotta Catch ‘Em All</em>: <a href="https://elitedatascience.com/python-seaborn-tutorial">https://elitedatascience.com/python-seaborn-tutorial</a>.<br/></li>
<li>(website) -- --; <em>Building a logistic regression classifier from the ground up</em>: <a href="http://inmachineswetrust.com/posts/building-logistic-regression/">http://inmachineswetrust.com/posts/building-logistic-regression/</a>.<br/></li>
<li>(website) -- --; <em>Agglomerative Clustering Essentials</em>: <a href="http://www.sthda.com/english/articles/28-hierarchical-clustering-essentials/90-agglomerative-clustering-essentials/">http://www.sthda.com/english/articles/28-hierarchical-clustering-essentials/90-agglomerative-clustering-essentials/</a>.<br/></li>
<li>(website) -- --; <em>Introduction to Information Retrieval | Online webpage : Hierarchical agglomerative clustering</em>: <a href="https://nlp.stanford.edu/IR-book/html/htmledition/hierarchical-clustering-1.html">https://nlp.stanford.edu/IR-book/html/htmledition/hierarchical-clustering-1.html</a>.<br/></li>
<li>(website) -- --; <em>Principles For Effective Data Visualization</em>: <a href="https://blog.zingchart.com/2014/09/08/principles-effective-data-visualization/">https://blog.zingchart.com/2014/09/08/principles-effective-data-visualization/</a>.<br/></li>
<li>(website) -- --; <em>Best Examples of Charts, Infographics and Maps</em>: <a href="https://infogram.com/examples">https://infogram.com/examples</a>.<br/></li>
<li>(website) -- --; <em>Towards Data Science Blog | Data Visualization</em>: <a href="https://towardsdatascience.com/tagged/data-visualization">https://towardsdatascience.com/tagged/data-visualization</a>.<br/></li>
<li>(website) -- --; <em>Choose the Right Chart Type for your Data</em>: <a href="https://www.labnol.org/software/find-right-chart-type-for-your-data/6523/">https://www.labnol.org/software/find-right-chart-type-for-your-data/6523/</a>.<br/></li>
<li>(website) -- --; <em>Data Visualization for Human Perception</em>: <a href="https://www.interaction-design.org/literature/book/the-encyclopedia-of-human-computer-interaction-2nd-ed/data-visualization-for-human-perception">https://www.interaction-design.org/literature/book/the-encyclopedia-of-human-computer-interaction-2nd-ed/data-visualization-for-human-perception</a>.<br/></li>
<li>2012. Barber, D.; <a href="http://web4.cs.ucl.ac.uk/staff/D.Barber/textbook/020217.pdf"><em>{Bayesian Reasoning and Machine Learning}</em></a>.<br/></li>
</ol>

</main>

  <footer>
  <script type="text/x-mathjax-config">
MathJax.Hub.Config({
  TeX: { equationNumbers: { autoNumber: "AMS" } }
});
</script>

  

  </footer>
  </body>
</html>

